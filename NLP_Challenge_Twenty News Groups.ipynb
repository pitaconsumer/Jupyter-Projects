{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Extraction Challenge\n",
    "## Comparing LSA, LDA, and NNMF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method:¶\n",
    "Parse and process the data into a tf-idf matrix.\n",
    "Fit LSA, LDA, and NNMF models with 5 topics each.\n",
    "Extract the words that best describe each topic.\n",
    "Examine the topic relationships for top words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "np.random.seed(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up default plotting parameters\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20.0, 7.0]\n",
    "plt.rcParams.update({'font.size': 22,})\n",
    "\n",
    "sns.set_palette('Set2')\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk', font_scale=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data set - training data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(shuffle=True, remove=('headers', 'footers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>A fair number of brave souls who upgraded thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>well folks, my mac plus finally gave up the gh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Robert J.C. Kyanko (rob@rjck.UUCP) wrote:\\n&gt; a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>From article &lt;C5owCB.n3p@world.std.com&gt;, by to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11309</td>\n",
       "      <td>13</td>\n",
       "      <td>DN&gt; From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11310</td>\n",
       "      <td>4</td>\n",
       "      <td>I have a (very old) Mac 512k and a Mac Plus, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11311</td>\n",
       "      <td>3</td>\n",
       "      <td>I just installed a DX2-66 CPU in a clone mothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11312</td>\n",
       "      <td>1</td>\n",
       "      <td>In article &lt;1qkgbuINNs9n@shelley.u.washington....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11313</td>\n",
       "      <td>8</td>\n",
       "      <td>Stolen from Pasadena between 4:30 and 6:30 pm ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target                                               data\n",
       "0           7  I was wondering if anyone out there could enli...\n",
       "1           4  A fair number of brave souls who upgraded thei...\n",
       "2           4  well folks, my mac plus finally gave up the gh...\n",
       "3           1  Robert J.C. Kyanko (rob@rjck.UUCP) wrote:\\n> a...\n",
       "4          14  From article <C5owCB.n3p@world.std.com>, by to...\n",
       "...       ...                                                ...\n",
       "11309      13  DN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...\n",
       "11310       4  I have a (very old) Mac 512k and a Mac Plus, b...\n",
       "11311       3  I just installed a DX2-66 CPU in a clone mothe...\n",
       "11312       1  In article <1qkgbuINNs9n@shelley.u.washington....\n",
       "11313       8  Stolen from Pasadena between 4:30 and 6:30 pm ...\n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Name Data frame from 20 Newsgroups\n",
    "news_df = pd.DataFrame(newsgroups.data, newsgroups.target).reset_index()\n",
    "news_df.columns = ['target', 'data']\n",
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation: WE have 2 columns of labelled data with 11,314 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Locate row 0 in column 1 (column 1= 'data')\n",
    "sample_data = news_df.iloc[0,1]\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'data' colum shows all the text without cleaning. Let's clean them.\n",
    "\n",
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(data):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = data.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', data)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', data)\n",
    "    text = re.sub('\\w*\\d\\w*', '', data)\n",
    "    return text\n",
    "\n",
    "cleaned_text = lambda x: clean_text_round1(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-20-f40eff4fa2fd>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-f40eff4fa2fd>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    df_real2 = pd.DataFrame(cleaned_text.apply(clean_text)\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#Let's take a look at the updated text\n",
    "df_real2 = pd.DataFrame(cleaned_text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",  \n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return c_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [line, ducati, gts, model, clock, run, paint, ...\n",
       "1    [article, freenet, carleton, freenet, carleton...\n",
       "2                                                   []\n",
       "3    [article, kjenks, gothamcity, jsc, nasa, gov, ...\n",
       "4    [reduce, price, list, thing, forsale, behalf, ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_numeric\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces, strip_non_alphanum, remove_stopwords, strip_short\n",
    "\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(), #lowercase\n",
    "                  strip_tags, # remove html tags\n",
    "                  strip_punctuation, # replace punctuation with space\n",
    "                  strip_multiple_whitespaces,# remove repeating whitespaces\n",
    "                  strip_non_alphanum, # remove non-alphanumeric characters\n",
    "                  strip_numeric, # remove numbers\n",
    "                  remove_stopwords,# remove stopwords\n",
    "                  strip_short # remove words less than minsize=3 characters long\n",
    "                 ]\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def gensim_preprocess(docs, logging=True):\n",
    "    docs = [expandContractions(doc) for doc in docs]\n",
    "    docs = [preprocess_string(text, CUSTOM_FILTERS) for text in docs]\n",
    "    texts_out = []\n",
    "    for doc in docs:\n",
    "    # https://spacy.io/usage/processing-pipelines\n",
    "        doc = nlp((\" \".join(doc)),  # doc = text to tokenize => creates doc\n",
    "                  # disable parts of the language processing pipeline we don't need here to speed up processing\n",
    "                  disable=['ner', # named entity recognition\n",
    "                           'tagger', # part-of-speech tagger\n",
    "                           'textcat', # document label categorizer\n",
    "                          ])\n",
    "        texts_out.append([tok.lemma_ for tok in doc if tok.lemma_ != '-PRON-'])\n",
    "    return pd.Series(texts_out)\n",
    "\n",
    "gensim_preprocess(news_df.data.iloc[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = gensim_preprocess(news_df.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wonder enlighten car see day door sport car look late early call bricklin door small addition bumper separate rest body know tellme model engine spec year production car history info funky look car mail'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "texts = [' '.join(text) for text in texts]\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn tfidf\n",
    "vectorizer = TfidfVectorizer()\n",
    "sklearn_tfidf = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Getting the word list.\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# Number of topics.\n",
    "ntopics=len(newsgroups.target_names)\n",
    "\n",
    "# Linking words to topics\n",
    "def word_topic(tfidf, solution, wordlist):\n",
    "    \n",
    "    # Loading scores for each word on each topic/component.\n",
    "    words_by_topic=tfidf.T * solution\n",
    "\n",
    "    # Linking the loadings to the words in an easy-to-read way.\n",
    "    components=pd.DataFrame(words_by_topic,index=wordlist)\n",
    "    \n",
    "    return components\n",
    "\n",
    "# Extracts the top N words and their loadings for each topic.\n",
    "def top_words(components, n_top_words):\n",
    "    n_topics = range(components.shape[1])\n",
    "    index= np.repeat(n_topics, n_top_words, axis=0)\n",
    "    topwords=pd.Series(index=index)\n",
    "    for column in range(components.shape[1]):\n",
    "        # Sort the column so that highest loadings are at the top.\n",
    "        sortedwords=components.iloc[:,column].sort_values(ascending=False)\n",
    "        # Choose the N highest loadings.\n",
    "        chosen=sortedwords[:n_top_words]\n",
    "        # Combine loading and index into a string.\n",
    "        chosenlist=chosen.index +\" \"+round(chosen,2).map(str) \n",
    "        topwords.loc[column]=chosenlist\n",
    "    return(topwords)\n",
    "\n",
    "# Number of words to look at for each topic.\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x63947 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 820866 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "svd= TruncatedSVD(ntopics)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "lsa = lsa.fit_transform(sklearn_tfidf)\n",
    "\n",
    "components_lsa = word_topic(sklearn_tfidf, lsa, terms)\n",
    "\n",
    "topwords=pd.DataFrame()\n",
    "topwords['LSA']=top_words(components_lsa, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     NaN\n",
       "0     NaN\n",
       "0     NaN\n",
       "0     NaN\n",
       "0     NaN\n",
       "     ... \n",
       "19    NaN\n",
       "19    NaN\n",
       "19    NaN\n",
       "19    NaN\n",
       "19    NaN\n",
       "Length: 200, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words(components_lsa, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 557 ms, total: 12.5 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(alpha=0.0, \n",
    "          init='nndsvdar', # how starting value are calculated\n",
    "          l1_ratio=0.0, # Sets whether regularization is L2 (0), L1 (1), or a combination (values between 0 and 1)\n",
    "          max_iter=200, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          n_components=ntopics, \n",
    "          random_state=0, \n",
    "          solver='cd', # Use Coordinate Descent to solve\n",
    "          tol=0.0001, # model will stop if tfidf-WH <= tol\n",
    "          verbose=0 # amount of output to give while iterating\n",
    "         )\n",
    "%time nmf = nmf.fit_transform(sklearn_tfidf) \n",
    "\n",
    "components_nmf = word_topic(sklearn_tfidf, nmf, terms)\n",
    "\n",
    "topwords['NNMF']=top_words(components_nmf, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "   LSA NNMF\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "Topic 1:\n",
      "   LSA NNMF\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "Topic 2:\n",
      "   LSA NNMF\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "Topic 3:\n",
      "   LSA NNMF\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "Topic 4:\n",
      "   LSA NNMF\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "Topic 5:\n",
      "   LSA NNMF\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "Topic 6:\n",
      "   LSA NNMF\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "Topic 7:\n",
      "   LSA NNMF\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "Topic 8:\n",
      "   LSA NNMF\n",
      "8  NaN  NaN\n",
      "8  NaN  NaN\n",
      "8  NaN  NaN\n",
      "8  NaN  NaN\n",
      "8  NaN  NaN\n",
      "8  NaN  NaN\n",
      "8  NaN  NaN\n",
      "8  NaN  NaN\n",
      "8  NaN  NaN\n",
      "8  NaN  NaN\n",
      "Topic 9:\n",
      "   LSA NNMF\n",
      "9  NaN  NaN\n",
      "9  NaN  NaN\n",
      "9  NaN  NaN\n",
      "9  NaN  NaN\n",
      "9  NaN  NaN\n",
      "9  NaN  NaN\n",
      "9  NaN  NaN\n",
      "9  NaN  NaN\n",
      "9  NaN  NaN\n",
      "9  NaN  NaN\n",
      "Topic 10:\n",
      "    LSA NNMF\n",
      "10  NaN  NaN\n",
      "10  NaN  NaN\n",
      "10  NaN  NaN\n",
      "10  NaN  NaN\n",
      "10  NaN  NaN\n",
      "10  NaN  NaN\n",
      "10  NaN  NaN\n",
      "10  NaN  NaN\n",
      "10  NaN  NaN\n",
      "10  NaN  NaN\n",
      "Topic 11:\n",
      "    LSA NNMF\n",
      "11  NaN  NaN\n",
      "11  NaN  NaN\n",
      "11  NaN  NaN\n",
      "11  NaN  NaN\n",
      "11  NaN  NaN\n",
      "11  NaN  NaN\n",
      "11  NaN  NaN\n",
      "11  NaN  NaN\n",
      "11  NaN  NaN\n",
      "11  NaN  NaN\n",
      "Topic 12:\n",
      "    LSA NNMF\n",
      "12  NaN  NaN\n",
      "12  NaN  NaN\n",
      "12  NaN  NaN\n",
      "12  NaN  NaN\n",
      "12  NaN  NaN\n",
      "12  NaN  NaN\n",
      "12  NaN  NaN\n",
      "12  NaN  NaN\n",
      "12  NaN  NaN\n",
      "12  NaN  NaN\n",
      "Topic 13:\n",
      "    LSA NNMF\n",
      "13  NaN  NaN\n",
      "13  NaN  NaN\n",
      "13  NaN  NaN\n",
      "13  NaN  NaN\n",
      "13  NaN  NaN\n",
      "13  NaN  NaN\n",
      "13  NaN  NaN\n",
      "13  NaN  NaN\n",
      "13  NaN  NaN\n",
      "13  NaN  NaN\n",
      "Topic 14:\n",
      "    LSA NNMF\n",
      "14  NaN  NaN\n",
      "14  NaN  NaN\n",
      "14  NaN  NaN\n",
      "14  NaN  NaN\n",
      "14  NaN  NaN\n",
      "14  NaN  NaN\n",
      "14  NaN  NaN\n",
      "14  NaN  NaN\n",
      "14  NaN  NaN\n",
      "14  NaN  NaN\n",
      "Topic 15:\n",
      "    LSA NNMF\n",
      "15  NaN  NaN\n",
      "15  NaN  NaN\n",
      "15  NaN  NaN\n",
      "15  NaN  NaN\n",
      "15  NaN  NaN\n",
      "15  NaN  NaN\n",
      "15  NaN  NaN\n",
      "15  NaN  NaN\n",
      "15  NaN  NaN\n",
      "15  NaN  NaN\n",
      "Topic 16:\n",
      "    LSA NNMF\n",
      "16  NaN  NaN\n",
      "16  NaN  NaN\n",
      "16  NaN  NaN\n",
      "16  NaN  NaN\n",
      "16  NaN  NaN\n",
      "16  NaN  NaN\n",
      "16  NaN  NaN\n",
      "16  NaN  NaN\n",
      "16  NaN  NaN\n",
      "16  NaN  NaN\n",
      "Topic 17:\n",
      "    LSA NNMF\n",
      "17  NaN  NaN\n",
      "17  NaN  NaN\n",
      "17  NaN  NaN\n",
      "17  NaN  NaN\n",
      "17  NaN  NaN\n",
      "17  NaN  NaN\n",
      "17  NaN  NaN\n",
      "17  NaN  NaN\n",
      "17  NaN  NaN\n",
      "17  NaN  NaN\n",
      "Topic 18:\n",
      "    LSA NNMF\n",
      "18  NaN  NaN\n",
      "18  NaN  NaN\n",
      "18  NaN  NaN\n",
      "18  NaN  NaN\n",
      "18  NaN  NaN\n",
      "18  NaN  NaN\n",
      "18  NaN  NaN\n",
      "18  NaN  NaN\n",
      "18  NaN  NaN\n",
      "18  NaN  NaN\n",
      "Topic 19:\n",
      "    LSA NNMF\n",
      "19  NaN  NaN\n",
      "19  NaN  NaN\n",
      "19  NaN  NaN\n",
      "19  NaN  NaN\n",
      "19  NaN  NaN\n",
      "19  NaN  NaN\n",
      "19  NaN  NaN\n",
      "19  NaN  NaN\n",
      "19  NaN  NaN\n",
      "19  NaN  NaN\n"
     ]
    }
   ],
   "source": [
    "for topic in range(ntopics):\n",
    "    print('Topic {}:'.format(topic))\n",
    "    print(topwords.loc[topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Observation: What words to target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'components_lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-dfff4ecc49c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargetwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     loadings=components_lsa.loc[word].append(\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mcomponents_lda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             components_nmf.loc[word])\n\u001b[1;32m     12\u001b[0m     \u001b[0mwordloadings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloadings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'components_lda' is not defined"
     ]
    }
   ],
   "source": [
    "# The words to look at.\n",
    "targetwords=['write','encryption','game', 'god', 'driver']\n",
    "\n",
    "# Storing the loadings.\n",
    "wordloadings=pd.DataFrame(columns=targetwords)\n",
    "\n",
    "# For each word, extracting and string the loadings for each method.\n",
    "for word in targetwords:\n",
    "    loadings=components_lsa.loc[word].append(\n",
    "        components_lda.loc[word]).append(\n",
    "            components_nmf.loc[word])\n",
    "    wordloadings[word]=loadings\n",
    "\n",
    "# Labeling the data by method and providing an ordering variable for graphing purposes. \n",
    "wordloadings['method']=np.repeat(['LSA','LDA','NNMF'], len(newsgroups.target_names), axis=0)\n",
    "wordloadings['loading']=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]*3\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "for word in targetwords:\n",
    "    sns.barplot(x=\"method\", y=word, hue=\"loading\", data=wordloadings)\n",
    "    plt.title(word)\n",
    "    plt.ylabel(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSA is the method most likely to have high loadings on more than one topic for the same word. LDA tends to have one high loading and some lower loadings. Loadings for NNMF are lower all around, and the most sparse, with some of the topics having loadings of zero on each word.\n",
    "\n",
    "According to NLP for Hackers topic modeling is:\n",
    "\n",
    "Dimensinality Reduction - We reduce dimensionality by representing a text in its topic space instead of its word space.\n",
    "Unsupervised Learning - Topic modeling is similar to clustering.\n",
    "A Form of Tagging - Topic modeling applys multiple tags to a text. (Similar to the tags applied to this kernel above!)\n",
    "Topic modeling is useful for many situations, including text classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
