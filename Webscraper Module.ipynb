{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Meetup\n",
    "\n",
    "## Findings:\n",
    "1) The average number of people RSVPed for a Meetup event was 14.\n",
    "2) Half of the Meetup events had a number of RSVPs between 4 and 8.\n",
    "3) There are some words that come up more often than others in the event titles (women, Denver, happy hour, networking, meditation, code). However, the presence of certain words seems to change based on the time of the week the data is scraped. For example, once Friday events were included in the data I scraped, a common word was \"poker.\" \"Poker\" did not show up at all in the Thursday events. The opposite was true of the word \"code.\"\n",
    "4) Overall, average number of RSVPs was higher when the words \"happy hour\" or \"networking\" were included in the event title.\n",
    "5) However, the events with \"meditation\" in the event title had very low relative attendance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class MUSpider(scrapy.Spider):\n",
    "\n",
    "    name = \"MUS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.meetup.com/find/events/?allMeetups=true&radius=5&userFreeform=Denver%2C+CO&mcId=z80212&mcName=Denver%2C+CO&eventFilter=all',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for event in response.xpath('//*[@class=\"row event-listing clearfix doc-padding  \"]'):\n",
    "            \n",
    "  \n",
    "            yield {\n",
    "\n",
    "                \"meetup_title\": event.xpath(\"div[2]/div/a/span[@itemprop='name']/text()\").extract(),\n",
    "                \"time\": event.xpath(\"div[1]/a/time/@datetime\").extract(),\n",
    "                \"organization\": event.xpath(\"div[2]/div/div[1]/a/span[@itemprop='name']/text()\").extract(),\n",
    "                \"num_attending\": event.xpath(\"div[2]/div/div[2]/div[@class='attendee-count']/text()\").extract_first()\n",
    "              \n",
    "            }\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'Meetups22.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False          \n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(MUSpider)\n",
    "process.start()\n",
    "print('Success!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "firstpage = pd.read_json('Meetups22.json', orient='records')\n",
    "print(firstpage.shape)\n",
    "firstpage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_numbers(string):\n",
    "    for s in string.split():\n",
    "        if s.isdigit():\n",
    "            int(s)\n",
    "            return(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage[\"num_attending\"] = firstpage[\"num_attending\"].apply(pull_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage[\"meetup_title\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(lst):\n",
    "    return lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage[\"meetup_title\"] = firstpage[\"meetup_title\"].apply(list_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage.iloc[65:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage[\"num_attending\"] = firstpage[\"num_attending\"].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statistics\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(firstpage[\"num_attending\"], color=\"purple\", bins=20)\n",
    "plt.title(\"Number of People RSVPed for Meetup Event\")\n",
    "plt.ylabel(\"Number of Meetup Events\")\n",
    "plt.xlabel(\"Number of RSVPs\")\n",
    "plt.axvline(statistics.mean(firstpage[\"num_attending\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage[\"Women\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_woman(string):\n",
    "    if \"Women\" in string:\n",
    "        return firstpage[\"Women\"] == 1\n",
    "    else:\n",
    "        return firstpage[\"Women\"] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage[\"Women\"] = firstpage[\"meetup_title\"].apply(word_woman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_denver(string):\n",
    "    if \"Denver\" in string:\n",
    "        return firstpage[\"Denver\"] == 1\n",
    "    else:\n",
    "        return firstpage[\"Denver\"] == 0\n",
    "    \n",
    "def word_code(string):\n",
    "    if \"Code\" in string:\n",
    "        return firstpage[\"Code\"] == 1\n",
    "    else:\n",
    "        return firstpage[\"Code\"] == 0\n",
    "\n",
    "def word_hh(string):\n",
    "    if \"Happy Hour\" in string:\n",
    "        return firstpage[\"Happy_Hour\"] == 1\n",
    "    else:\n",
    "        return firstpage[\"Happy_Hour\"] == 0\n",
    "    \n",
    "def word_meditation(string):\n",
    "    if \"Meditation\" in string:\n",
    "        return firstpage[\"Meditation\"] == 1\n",
    "    else:\n",
    "        return firstpage[\"Meditation\"] == 0\n",
    "    \n",
    "def word_networking(string):\n",
    "    if \"Networking\" in string:\n",
    "        return firstpage[\"Networking\"] == 1\n",
    "    else:\n",
    "        return firstpage[\"Networking\"] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage[\"Denver\"] = 0\n",
    "firstpage[\"Denver\"] = firstpage[\"meetup_title\"].apply(word_denver)\n",
    "\n",
    "firstpage[\"Code\"] = 0\n",
    "firstpage[\"Code\"] = firstpage[\"meetup_title\"].apply(word_code)\n",
    "\n",
    "firstpage[\"Happy_Hour\"] = 0\n",
    "firstpage[\"Happy_Hour\"] = firstpage[\"meetup_title\"].apply(word_hh)\n",
    "\n",
    "firstpage[\"Meditation\"] = 0\n",
    "firstpage[\"Meditation\"] = firstpage[\"meetup_title\"].apply(word_meditation)\n",
    "\n",
    "firstpage[\"Networking\"] = 0\n",
    "firstpage[\"Networking\"] = firstpage[\"meetup_title\"].apply(word_networking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = {False: 1, True: 0}\n",
    "\n",
    "words = [\"Women\", \"Denver\", \"Code\", \"Happy_Hour\", \"Meditation\", \"Networking\"]\n",
    "\n",
    "for word in words:\n",
    "    firstpage[word] = firstpage[word].map(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for col in firstpage.columns[4:]:\n",
    "    plt.bar(x=col, height=sum(firstpage[col]))\n",
    "plt.title(\"Most Frequent Words In Event Title\", size=15)\n",
    "plt.ylabel(\"Frequency in Event Titles\", size=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_words(df):\n",
    "    i = 0\n",
    "    if df[\"Women\"] == 1:\n",
    "        i += 1\n",
    "    if df[\"Denver\"] == 1:\n",
    "        i += 1\n",
    "    if df[\"Code\"] == 1:\n",
    "        i += 1\n",
    "    if df[\"Happy_Hour\"] == 1:\n",
    "        i += 1\n",
    "    if df[\"Meditation\"] == 1:\n",
    "        i += 1\n",
    "    if df[\"Networking\"] == 1:\n",
    "        i += 1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage[\"Hot_Words\"] = firstpage.apply(hot_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_by_num_of_hot_words = []\n",
    "\n",
    "for n in [0, 1, 2, 3]:\n",
    "    avg = sum(firstpage[firstpage[\"Hot_Words\"] == n][\"num_attending\"])/ (firstpage[firstpage[\"Hot_Words\"] == n].shape[0] + .0001)\n",
    "    averages_by_num_of_hot_words.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_by_num_of_hot_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(x=[0, 1, 2, 3], height=averages_by_num_of_hot_words)\n",
    "plt.xticks([0, 1, 2, 3])\n",
    "plt.title(\"Average Attendance By Number of Hot Words\", size=20)\n",
    "plt.xlabel(\"Number of Hot Words\", size=15)\n",
    "plt.ylabel(\"Average Attendance\", size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "for col in firstpage.columns[4:10]:\n",
    "\n",
    "    if sum(firstpage[col]) == 0:\n",
    "        del firstpage[col]\n",
    "        n -=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_hot_word_attendance = []\n",
    "without_hot_word_attendance = []\n",
    "\n",
    "for col in firstpage.columns[4:n]:\n",
    "    with_avg = statistics.mean(firstpage[firstpage[col] == 1][\"num_attending\"]) \n",
    "    without_avg = statistics.mean(firstpage[firstpage[col] == 0][\"num_attending\"])\n",
    "    with_hot_word_attendance.append(with_avg)\n",
    "    without_hot_word_attendance.append(without_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "ind = np.arange(n-4)    # the x locations for the groups\n",
    "width = 0.35         # the width of the bars\n",
    "p1 = ax.bar(ind, with_hot_word_attendance, width, color='green')\n",
    "\n",
    "\n",
    "p2 = ax.bar(ind + width, without_hot_word_attendance, width, color='yellow')\n",
    "\n",
    "ax.set_title('Average Attendance By Presence/Absence of Hot Words')\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels((firstpage.columns[4:n]), fontsize=15)\n",
    "ax.legend((p1[0], p2[0]), ('With Hot Word', 'Without Hot Word'), fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Middle East Forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class MEFSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"MEF\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.meforum.org',\n",
    "    ]\n",
    "\n",
    "    # What to do with the URL.  Here, we tell it to download all the code and save\n",
    "    # it to the mainpage.html file\n",
    "    def parse(self, response):\n",
    "        with open('mainpage.html', 'wb') as f:\n",
    "            f.write(response.body)\n",
    "\n",
    "\n",
    "# Instantiate our crawler.\n",
    "process = CrawlerProcess()\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(MEFSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "### Note:\n",
    "I should now have a file called 'mainpage.html' saved to your machine that contains all the code from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class MEFSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"MEF\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.meforum.org/',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for article in response.xpath('//article'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
    "                'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'firstpage.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(MEFSpider)\n",
    "process.start()\n",
    "print('Success!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import scrapy.crawler as crawler\n",
    "from multiprocessing import Process, Queue\n",
    "from twisted.internet import reactor\n",
    "\n",
    "# your spider\n",
    "class MEFSpider(scrapy.Spider):\n",
    "    name = \"MEF\"\n",
    "    start_urls = ['https://www.meforum.org/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            print(quote.css('span.text::text').extract_first())\n",
    "\n",
    "\n",
    "# the wrapper to make it run more times\n",
    "def run_spider(spider):\n",
    "    def f(q):\n",
    "        try:\n",
    "            runner = crawler.CrawlerRunner()\n",
    "            deferred = runner.crawl(spider)\n",
    "            deferred.addBoth(lambda _: reactor.stop())\n",
    "            reactor.run()\n",
    "            q.put(None)\n",
    "        except Exception as e:\n",
    "            q.put(e)\n",
    "\n",
    "    q = Queue()\n",
    "    p = Process(target=f, args=(q,))\n",
    "    p.start()\n",
    "    result = q.get()\n",
    "    p.join()\n",
    "\n",
    "    if result is not None:\n",
    "        raise result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class MEFSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"MEF\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.meforum.org/',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for article in response.xpath('//article'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
    "                'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'firstpage.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(MEFSpider)\n",
    "process.start()\n",
    "print('Success!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Checking whether we got data from all 9 pages\n",
    "MEFdf=pd.read_json('data.json', orient='records')\n",
    "print(MEFdf.shape)\n",
    "print(MEFdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
