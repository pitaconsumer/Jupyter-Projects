{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Fake-Authentic Classifier Over Facebook Political Ads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem:\n",
    "\n",
    "## Question: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Learning a Classifier Model from Articles Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-23 20:58:51--  https://github.com/pitaconsumer/some-datasets/blob/master/572515_1037534_compressed_Fake.csv.zip?raw=true\n",
      "Resolving github.com (github.com)... 192.30.255.113\n",
      "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/pitaconsumer/some-datasets/raw/master/572515_1037534_compressed_Fake.csv.zip [following]\n",
      "--2020-07-23 20:58:52--  https://github.com/pitaconsumer/some-datasets/raw/master/572515_1037534_compressed_Fake.csv.zip\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/pitaconsumer/some-datasets/master/572515_1037534_compressed_Fake.csv.zip [following]\n",
      "--2020-07-23 20:58:52--  https://raw.githubusercontent.com/pitaconsumer/some-datasets/master/572515_1037534_compressed_Fake.csv.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.128.133, 151.101.64.133, 151.101.0.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.128.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n",
      "--2020-07-23 20:58:53--  https://github.com/pitaconsumer/some-datasets/blob/master/572515_1037534_compressed_True.csv.zip?raw=true\n",
      "Resolving github.com (github.com)... 192.30.255.113\n",
      "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/pitaconsumer/some-datasets/raw/master/572515_1037534_compressed_True.csv.zip [following]\n",
      "--2020-07-23 20:58:53--  https://github.com/pitaconsumer/some-datasets/raw/master/572515_1037534_compressed_True.csv.zip\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/pitaconsumer/some-datasets/master/572515_1037534_compressed_True.csv.zip [following]\n",
      "--2020-07-23 20:58:53--  https://raw.githubusercontent.com/pitaconsumer/some-datasets/master/572515_1037534_compressed_True.csv.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.128.133, 151.101.0.133, 151.101.64.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.128.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n",
      "Archive:  572515_1037534_compressed_Fake.csv.zip?raw=true\n",
      "  inflating: Fake.csv                \n",
      "Archive:  572515_1037534_compressed_True.csv.zip?raw=true\n",
      "  inflating: True.csv                \n"
     ]
    }
   ],
   "source": [
    "!wget -c 'https://github.com/pitaconsumer/some-datasets/blob/master/572515_1037534_compressed_Fake.csv.zip?raw=true'\n",
    "!wget -c \"https://github.com/pitaconsumer/some-datasets/blob/master/572515_1037534_compressed_True.csv.zip?raw=true\"\n",
    "!unzip -o  \"572515_1037534_compressed_Fake.csv.zip?raw=true\"\n",
    "!unzip -o \"572515_1037534_compressed_True.csv.zip?raw=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (364 kB)\n",
      "\u001b[K     |████████████████████████████████| 364 kB 337 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from wordcloud) (1.18.5)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud) (7.1.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from wordcloud) (3.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import en_core_web_lg\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress bar\")\n",
    "\n",
    "\n",
    "import preprocessor\n",
    "from textblob import TextBlob\n",
    "import statistics\n",
    "from typing import List\n",
    "\n",
    "\n",
    "import scipy\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df = pd.read_csv('Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 µs, sys: 0 ns, total: 25 µs\n",
      "Wall time: 28.8 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23481, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time fake_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df = pd.read_csv(\"True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress bar: 100%|██████████| 23481/23481 [21:27<00:00, 18.24it/s]\n"
     ]
    }
   ],
   "source": [
    "fake_vectors = fake_df['text'].progress_apply(lambda x: pd.Series(nlp(x).doc.vector.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress bar: 100%|██████████| 21417/21417 [18:04<00:00, 19.75it/s]\n"
     ]
    }
   ],
   "source": [
    "true_vectors = true_df['text'].progress_apply(lambda x: pd.Series(nlp(x).doc.vector.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_vectors['y'] = 0\n",
    "true_vectors['y'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_vectors.to_pickle('fake_vectors.pickle')\n",
    "true_vectors.to_pickle('true_vectors.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df['y'] = 1\n",
    "fake_df['y'] = 0\n",
    "\n",
    "all_df= pd.concat([true_df, fake_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>McPain: John McCain Furious That Iran Treated ...</td>\n",
       "      <td>21st Century Wire says As 21WIRE reported earl...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 16, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>JUSTICE? Yahoo Settles E-mail Privacy Class-ac...</td>\n",
       "      <td>21st Century Wire says It s a familiar theme. ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 16, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>Sunnistan: US and Allied ‘Safe Zone’ Plan to T...</td>\n",
       "      <td>Patrick Henningsen  21st Century WireRemember ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 15, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>How to Blow $700 Million: Al Jazeera America F...</td>\n",
       "      <td>21st Century Wire says Al Jazeera America will...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 14, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>10 U.S. Navy Sailors Held by Iranian Military ...</td>\n",
       "      <td>21st Century Wire says As 21WIRE predicted in ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 12, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      As U.S. budget fight looms, Republicans flip t...   \n",
       "1      U.S. military to accept transgender recruits o...   \n",
       "2      Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3      FBI Russia probe helped by Australian diplomat...   \n",
       "4      Trump wants Postal Service to charge 'much mor...   \n",
       "...                                                  ...   \n",
       "44893  McPain: John McCain Furious That Iran Treated ...   \n",
       "44894  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...   \n",
       "44895  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...   \n",
       "44896  How to Blow $700 Million: Al Jazeera America F...   \n",
       "44897  10 U.S. Navy Sailors Held by Iranian Military ...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "0      WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1      WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2      WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3      WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4      SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "...                                                  ...           ...   \n",
       "44893  21st Century Wire says As 21WIRE reported earl...   Middle-east   \n",
       "44894  21st Century Wire says It s a familiar theme. ...   Middle-east   \n",
       "44895  Patrick Henningsen  21st Century WireRemember ...   Middle-east   \n",
       "44896  21st Century Wire says Al Jazeera America will...   Middle-east   \n",
       "44897  21st Century Wire says As 21WIRE predicted in ...   Middle-east   \n",
       "\n",
       "                     date  y  \n",
       "0      December 31, 2017   1  \n",
       "1      December 29, 2017   1  \n",
       "2      December 31, 2017   1  \n",
       "3      December 30, 2017   1  \n",
       "4      December 29, 2017   1  \n",
       "...                   ... ..  \n",
       "44893    January 16, 2016  0  \n",
       "44894    January 16, 2016  0  \n",
       "44895    January 15, 2016  0  \n",
       "44896    January 14, 2016  0  \n",
       "44897    January 12, 2016  0  \n",
       "\n",
       "[44898 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Bag of Words Method\n",
    "We will compare and contrast results of using Random Forest Classifier with BOW method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do I open pick from here?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Need to import library for Stop Words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize text for real news \n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    #pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    #token = word_tokenize(text)\n",
    "    words = [word for word in text.split(\" \") if word.isalpha()]\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.discard(\"not\")\n",
    "    PS = PorterStemmer()\n",
    "    words = [PS.stem(w) for w in words if w not in stop_words]\n",
    "    words = ' '.join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [washington, head, conserv, republican, factio...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(all_df['text'].head(1).apply(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "all_df['updated_text'] = all_df['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updated_text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>washington head conserv republican faction vot...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>washington transgend peopl allow first time en...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>washington special counsel investig link russi...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>washington trump campaign advis georg papadopo...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>presid donald trump call postal servic friday ...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>centuri wire say report earlier unlik mishap t...</td>\n",
       "      <td>21st Century Wire says As 21WIRE reported earl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>centuri wire say familiar whenev disput chang ...</td>\n",
       "      <td>21st Century Wire says It s a familiar theme. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>patrick henningsen centuri wirerememb obama ad...</td>\n",
       "      <td>Patrick Henningsen  21st Century WireRemember ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>centuri wire say al jazeera america go histori...</td>\n",
       "      <td>21st Century Wire says Al Jazeera America will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>centuri wire say predict new year look new hos...</td>\n",
       "      <td>21st Century Wire says As 21WIRE predicted in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            updated_text  \\\n",
       "0      washington head conserv republican faction vot...   \n",
       "1      washington transgend peopl allow first time en...   \n",
       "2      washington special counsel investig link russi...   \n",
       "3      washington trump campaign advis georg papadopo...   \n",
       "4      presid donald trump call postal servic friday ...   \n",
       "...                                                  ...   \n",
       "44893  centuri wire say report earlier unlik mishap t...   \n",
       "44894  centuri wire say familiar whenev disput chang ...   \n",
       "44895  patrick henningsen centuri wirerememb obama ad...   \n",
       "44896  centuri wire say al jazeera america go histori...   \n",
       "44897  centuri wire say predict new year look new hos...   \n",
       "\n",
       "                                                    text  \n",
       "0      WASHINGTON (Reuters) - The head of a conservat...  \n",
       "1      WASHINGTON (Reuters) - Transgender people will...  \n",
       "2      WASHINGTON (Reuters) - The special counsel inv...  \n",
       "3      WASHINGTON (Reuters) - Trump campaign adviser ...  \n",
       "4      SEATTLE/WASHINGTON (Reuters) - President Donal...  \n",
       "...                                                  ...  \n",
       "44893  21st Century Wire says As 21WIRE reported earl...  \n",
       "44894  21st Century Wire says It s a familiar theme. ...  \n",
       "44895  Patrick Henningsen  21st Century WireRemember ...  \n",
       "44896  21st Century Wire says Al Jazeera America will...  \n",
       "44897  21st Century Wire says As 21WIRE predicted in ...  \n",
       "\n",
       "[44898 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df[['updated_text', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "\n",
    "ignore_this = all_df['updated_text'].apply(lambda row: c.update(row.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trump', 102168),\n",
       " ('said', 93217),\n",
       " ('not', 78912),\n",
       " ('would', 54617),\n",
       " ('presid', 48109),\n",
       " ('state', 46619),\n",
       " ('peopl', 34875),\n",
       " ('republican', 34816),\n",
       " ('one', 34268),\n",
       " ('also', 30633),\n",
       " ('say', 30570),\n",
       " ('new', 30522),\n",
       " ('like', 28222),\n",
       " ('donald', 27063),\n",
       " ('democrat', 24948),\n",
       " ('unit', 24743),\n",
       " ('govern', 24724),\n",
       " ('hous', 24674),\n",
       " ('call', 23768),\n",
       " ('could', 23673),\n",
       " ('nation', 23665),\n",
       " ('told', 23203),\n",
       " ('support', 22743),\n",
       " ('make', 22692),\n",
       " ('go', 22408),\n",
       " ('report', 21961),\n",
       " ('clinton', 21918),\n",
       " ('obama', 21904),\n",
       " ('white', 21893),\n",
       " ('time', 20909),\n",
       " ('last', 20680),\n",
       " ('american', 20642),\n",
       " ('year', 20288),\n",
       " ('get', 20121),\n",
       " ('senat', 20017),\n",
       " ('use', 19858),\n",
       " ('includ', 19463),\n",
       " ('two', 19337),\n",
       " ('offici', 18983),\n",
       " ('want', 18751),\n",
       " ('take', 18538),\n",
       " ('campaign', 18196),\n",
       " ('polit', 18154),\n",
       " ('vote', 18111),\n",
       " ('elect', 17991),\n",
       " ('even', 17976),\n",
       " ('former', 17596),\n",
       " ('parti', 17574),\n",
       " ('news', 17495),\n",
       " ('first', 17437)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words = c.most_common(50)\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words_fake_real = vectorizer.fit_transform(all_df['updated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59069"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44898, 59069)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_fake_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44898,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words_fake_real, all_df['y'],\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30081,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_bag = RandomForestClassifier()\n",
    "\n",
    "rfc_bag.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_bag.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9718566511439563"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_bag.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud of Top 50 Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "?WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-ff20f326a495>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# most_common() produces k frequently encountered in: all_df['updated_text'](10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# input values and their respective counts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmost_occur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmost_occur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/collections/__init__.py\u001b[0m in \u001b[0;36mmost_common\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;31m# Emulate Bag.sortedByCount from Smalltalk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_itemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_heapq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_itemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "# most_common() produces k frequently encountered in: all_df['updated_text'](10) \n",
    "# input values and their respective counts. \n",
    "most_occur = Counter.most_common(top_words)\n",
    "\n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-6abedfd248a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                       \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                       \u001b[0mmax_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                       max_font_size =110).generate(top_words)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordcloud_BOW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"bilinear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \"\"\"\n\u001b[0;32m--> 631\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \"\"\"\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mregexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mr\"\\w[\\w']+\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         \u001b[0;31m# remove 's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         words = [word[:-2] if word.lower().endswith(\"'s\") else word\n",
      "\u001b[0;32m/usr/lib/python3.6/re.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "#Plot the Word Cloud\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create and generate a word cloud image: wordcloud = WordCloud().generate(text)\n",
    "wordcloud_BOW = WordCloud(width = 500, \n",
    "                      contour_color = \"purple\",\n",
    "                      height= 300, \n",
    "                      random_state = 21,\n",
    "                      max_words = 30,\n",
    "                      max_font_size =110).generate(top_words)\n",
    "plt.imshow(wordcloud_BOW, interpolation= \"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD TWO: NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Text Vector (spacy library) Method\n",
    "We will compare and contrast results of using Random Forest Classifier from BOW method with results from Text Vector method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TreebankTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3262bd54225f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTreebankWordTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTreebankTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TreebankTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import TreebankWordTokenizer\n",
    "tokenizer = TreebankTokenizer()\n",
    "train['tokens'] = train['text'].map(tokenizer.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors = pd.concat([fake_vectors, true_vectors], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 2 requires that X and y be trained as X2_, and y2_\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(all_vectors.drop(columns=['y']), \n",
    "                                                    all_vectors['y'], \n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30081,)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run a random forest classifier on vectors\n",
    "y2_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classifier for Method 2 using vectors\n",
    "rfc_vectors = RandomForestClassifier()\n",
    "\n",
    "rfc_vectors.fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999966756424321"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply classifier to X2 and y2 training set and obtain 'Score'.\n",
    "rfc_vectors.score(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9677397583856381"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply classifier to X2 and y2 testing set and obtain 'Score'.\n",
    "rfc_vectors.score(X2_test, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Facebook Political Ads Classified Into Fake Versus Authentic Via Random Forest Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLF from Training of fake data sets\n",
    "#### Predict outcomes for Facebook \n",
    "    Y_fb_pred = clf.fit(X_train_fake, Y_train_true).predict(X_test_fb_pol)\n",
    "clf.fit(X_fb_pol, Y_fb_pred).  #Test Set is FB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re,string,unicodedata\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "import os\n",
    "import gc\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import  Counter\n",
    "\n",
    "#stop = set(stopwords.words('english'))\n",
    "#punctuation = list(string.punctuation)\n",
    "#stop.update(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>html</th>\n",
       "      <th>political</th>\n",
       "      <th>not_political</th>\n",
       "      <th>title</th>\n",
       "      <th>message</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>lang</th>\n",
       "      <th>...</th>\n",
       "      <th>suppressed</th>\n",
       "      <th>targets</th>\n",
       "      <th>advertiser</th>\n",
       "      <th>entities</th>\n",
       "      <th>page</th>\n",
       "      <th>lower_page</th>\n",
       "      <th>targetings</th>\n",
       "      <th>paid_for_by</th>\n",
       "      <th>targetedness</th>\n",
       "      <th>listbuilding_fundraising_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hyperfeed_story_id_5c9baa3ee0ec08073500042</td>\n",
       "      <td>&lt;div class=\"_5pa- userContentWrapper\"&gt;&lt;div cla...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>League of Conservation Voters</td>\n",
       "      <td>&lt;p&gt;BREAKING: Trump’s Department of the Interio...</td>\n",
       "      <td>https://pp-facebook-ads.s3.amazonaws.com/v/t1....</td>\n",
       "      <td>2019-03-27 16:52:25.625455+00</td>\n",
       "      <td>2019-03-27 16:52:25.625455+00</td>\n",
       "      <td>en-US</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"entity\": \"Endangered Species Act\", \"entity_...</td>\n",
       "      <td>https://www.facebook.com/LCVoters/</td>\n",
       "      <td>https://www.facebook.com/lcvoters/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>League of Conservation Voters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.647945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hyperfeed_story_id_5c9bb2a2413852086735771</td>\n",
       "      <td>&lt;div class=\"_5pa- userContentWrapper\"&gt;&lt;div cla...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Indivisible Guide</td>\n",
       "      <td>&lt;p&gt;The Mueller investigation is over. Special ...</td>\n",
       "      <td>https://pp-facebook-ads.s3.amazonaws.com/v/t1....</td>\n",
       "      <td>2019-03-27 17:28:14.096849+00</td>\n",
       "      <td>2019-03-27 17:28:14.096849+00</td>\n",
       "      <td>en-US</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"entity\": \"Americans\", \"entity_type\": \"Group...</td>\n",
       "      <td>https://www.facebook.com/indivisibleguide/</td>\n",
       "      <td>https://www.facebook.com/indivisibleguide/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indivisible Project</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.350635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hyperfeed_story_id_5c9bb4fa461731e29426627</td>\n",
       "      <td>&lt;div class=\"_5pa- userContentWrapper\"&gt;&lt;div cla...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>International Rescue Committee</td>\n",
       "      <td>&lt;p&gt;Zimbabwe is reeling from the impact of Cycl...</td>\n",
       "      <td>https://pp-facebook-ads.s3.amazonaws.com/v/t1....</td>\n",
       "      <td>2019-03-27 17:38:23.101377+00</td>\n",
       "      <td>2019-03-27 17:38:23.101377+00</td>\n",
       "      <td>en-US</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"entity\": \"Zimbabwe\", \"entity_type\": \"Region\"}]</td>\n",
       "      <td>https://www.facebook.com/InternationalRescueCo...</td>\n",
       "      <td>https://www.facebook.com/internationalrescueco...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>International Rescue Committee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23843380741530360</td>\n",
       "      <td>&lt;div class=\"_5pcr userContentWrapper\"&gt;&lt;div cla...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Covenant House International</td>\n",
       "      <td>&lt;p&gt;What more can you do in the final hours of ...</td>\n",
       "      <td>https://pp-facebook-ads.s3.amazonaws.com/v/t1....</td>\n",
       "      <td>2018-12-30 20:59:13.879124+00</td>\n",
       "      <td>2018-12-30 20:59:13.879124+00</td>\n",
       "      <td>en-US</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>[{\"target\": \"Activity on the Facebook Family\"}...</td>\n",
       "      <td>Covenant House International</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.facebook.com/CovenantHouse/</td>\n",
       "      <td>https://www.facebook.com/covenanthouse/</td>\n",
       "      <td>{\"&lt;div&gt;&lt;div class=\\\"_4-i0 _26c5\\\"&gt;&lt;div class=\\...</td>\n",
       "      <td>Covenant House International</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyperfeed_story_id_5c9bb059454851c17741213</td>\n",
       "      <td>&lt;div class=\"_5pa- userContentWrapper\"&gt;&lt;div cla...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Planned Parenthood</td>\n",
       "      <td>&lt;p&gt;Say it loud, say it proud: Our rights, our ...</td>\n",
       "      <td>https://pp-facebook-ads.s3.amazonaws.com/v/t1....</td>\n",
       "      <td>2019-03-27 17:18:29.764002+00</td>\n",
       "      <td>2019-04-11 15:02:58.081112+00</td>\n",
       "      <td>en-US</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"entity\": \"Planned Parenthood\", \"entity_type...</td>\n",
       "      <td>https://www.facebook.com/PlannedParenthood/</td>\n",
       "      <td>https://www.facebook.com/plannedparenthood/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Planned Parenthood Federation of America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162319</th>\n",
       "      <td>23843108782710078</td>\n",
       "      <td>&lt;div class=\"_5pcr userContentWrapper\"&gt;&lt;div cla...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>Keep Them Accountable</td>\n",
       "      <td>&lt;p&gt;Rep. Katko voted for tax breaks for his wea...</td>\n",
       "      <td>https://pp-facebook-ads.s3.amazonaws.com/v/t1....</td>\n",
       "      <td>2018-10-19 10:31:52.466563+00</td>\n",
       "      <td>2018-10-22 11:40:06.24382+00</td>\n",
       "      <td>en-US</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>[{\"target\": \"Age\", \"segment\": \"18 and older\"},...</td>\n",
       "      <td>Keep Them Accountable</td>\n",
       "      <td>[{\"entity\": \"Katko\", \"entity_type\": \"Person\"}]</td>\n",
       "      <td>https://www.facebook.com/KeepThemAccountable18/</td>\n",
       "      <td>https://www.facebook.com/keepthemaccountable18/</td>\n",
       "      <td>{\"&lt;div&gt;&lt;div class=\\\"_4-i0 _26c5\\\"&gt;&lt;div class=\\...</td>\n",
       "      <td>HOUSE MAJORITY PAC, (202) 849-6052, AND PRIORI...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.116965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162320</th>\n",
       "      <td>23843034525850259</td>\n",
       "      <td>&lt;div class=\"_5pcr userContentWrapper\"&gt;&lt;div cla...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>National Republican Congressional Committee</td>\n",
       "      <td>&lt;p&gt;Illinois early voting is open NOW &amp;amp; you...</td>\n",
       "      <td>https://pp-facebook-ads.s3.amazonaws.com/v/t1....</td>\n",
       "      <td>2018-10-24 20:41:42.111865+00</td>\n",
       "      <td>2018-10-24 20:41:42.111865+00</td>\n",
       "      <td>en-US</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>[{\"target\": \"List\"}, {\"target\": \"Age\", \"segmen...</td>\n",
       "      <td>National Republican Congressional Committee</td>\n",
       "      <td>[{\"entity\": \"Illinois\", \"entity_type\": \"Region...</td>\n",
       "      <td>https://www.facebook.com/NRCC/</td>\n",
       "      <td>https://www.facebook.com/nrcc/</td>\n",
       "      <td>{\"&lt;div&gt;&lt;div class=\\\"_4-i0 _26c5\\\"&gt;&lt;div class=\\...</td>\n",
       "      <td>the NRCC and not authorized by any candidate o...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.312412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162321</th>\n",
       "      <td>23842997138670612</td>\n",
       "      <td>&lt;div class=\"_5pcr userContentWrapper\"&gt;&lt;div cla...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POW Action Fund</td>\n",
       "      <td>&lt;p&gt;From your favorite peaks to the polling pla...</td>\n",
       "      <td>https://pp-facebook-ads.s3.amazonaws.com/v/t1....</td>\n",
       "      <td>2018-10-09 20:03:32.81012+00</td>\n",
       "      <td>2018-10-09 20:03:32.81012+00</td>\n",
       "      <td>en-US</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>[{\"target\": \"Segment\", \"segment\": \"US politics...</td>\n",
       "      <td>POW Action Fund</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.facebook.com/POWActionFund/</td>\n",
       "      <td>https://www.facebook.com/powactionfund/</td>\n",
       "      <td>{\"&lt;div&gt;&lt;div class=\\\"_4-i0 _26c5\\\"&gt;&lt;div class=\\...</td>\n",
       "      <td>Protect Our Winters Action Fund</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.205220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162322</th>\n",
       "      <td>hyperfeed_story_id_5c8b16b11b8f86515960964</td>\n",
       "      <td>&lt;div class=\"_5pa- userContentWrapper\"&gt;&lt;div cla...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>Beto O'Rourke</td>\n",
       "      <td>&lt;p&gt;Beto just announced he’s running for presid...</td>\n",
       "      <td>https://pp-facebook-ads.s3.amazonaws.com/v/t1....</td>\n",
       "      <td>2019-03-15 03:07:40.590249+00</td>\n",
       "      <td>2019-03-22 17:01:05.36319+00</td>\n",
       "      <td>en-US</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"entity\": \"Beto\", \"entity_type\": \"Region\"}]</td>\n",
       "      <td>https://www.facebook.com/betoorourke/</td>\n",
       "      <td>https://www.facebook.com/betoorourke/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Beto for America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162323</th>\n",
       "      <td>23842885237930242</td>\n",
       "      <td>&lt;div class=\"_5pcr userContentWrapper\"&gt;&lt;div cla...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ACLU</td>\n",
       "      <td>&lt;p&gt;Claim your FREE ACLU Voter sticker today to...</td>\n",
       "      <td>https://pp-facebook-ads.s3.amazonaws.com/v/t1....</td>\n",
       "      <td>2018-08-08 02:54:51.076959+00</td>\n",
       "      <td>2018-08-08 20:03:36.302904+00</td>\n",
       "      <td>en-US</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>[{\"target\": \"Age\", \"segment\": \"35 to 54\"}, {\"t...</td>\n",
       "      <td>ACLU</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.facebook.com/aclu/</td>\n",
       "      <td>https://www.facebook.com/aclu/</td>\n",
       "      <td>{\"&lt;div&gt;&lt;div class=\\\"_4-i0 _26c5\\\"&gt;&lt;div class=\\...</td>\n",
       "      <td>the ACLU</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.354132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162324 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                id  \\\n",
       "0       hyperfeed_story_id_5c9baa3ee0ec08073500042   \n",
       "1       hyperfeed_story_id_5c9bb2a2413852086735771   \n",
       "2       hyperfeed_story_id_5c9bb4fa461731e29426627   \n",
       "3                                23843380741530360   \n",
       "4       hyperfeed_story_id_5c9bb059454851c17741213   \n",
       "...                                            ...   \n",
       "162319                           23843108782710078   \n",
       "162320                           23843034525850259   \n",
       "162321                           23842997138670612   \n",
       "162322  hyperfeed_story_id_5c8b16b11b8f86515960964   \n",
       "162323                           23842885237930242   \n",
       "\n",
       "                                                     html  political  \\\n",
       "0       <div class=\"_5pa- userContentWrapper\"><div cla...          0   \n",
       "1       <div class=\"_5pa- userContentWrapper\"><div cla...          0   \n",
       "2       <div class=\"_5pa- userContentWrapper\"><div cla...          0   \n",
       "3       <div class=\"_5pcr userContentWrapper\"><div cla...          0   \n",
       "4       <div class=\"_5pa- userContentWrapper\"><div cla...          0   \n",
       "...                                                   ...        ...   \n",
       "162319  <div class=\"_5pcr userContentWrapper\"><div cla...         12   \n",
       "162320  <div class=\"_5pcr userContentWrapper\"><div cla...          0   \n",
       "162321  <div class=\"_5pcr userContentWrapper\"><div cla...          0   \n",
       "162322  <div class=\"_5pa- userContentWrapper\"><div cla...          7   \n",
       "162323  <div class=\"_5pcr userContentWrapper\"><div cla...          0   \n",
       "\n",
       "        not_political                                        title  \\\n",
       "0                   0                League of Conservation Voters   \n",
       "1                   0                            Indivisible Guide   \n",
       "2                   0               International Rescue Committee   \n",
       "3                   0                 Covenant House International   \n",
       "4                   1                           Planned Parenthood   \n",
       "...               ...                                          ...   \n",
       "162319              0                        Keep Them Accountable   \n",
       "162320              0  National Republican Congressional Committee   \n",
       "162321              0                              POW Action Fund   \n",
       "162322              0                                Beto O'Rourke   \n",
       "162323              0                                         ACLU   \n",
       "\n",
       "                                                  message  \\\n",
       "0       <p>BREAKING: Trump’s Department of the Interio...   \n",
       "1       <p>The Mueller investigation is over. Special ...   \n",
       "2       <p>Zimbabwe is reeling from the impact of Cycl...   \n",
       "3       <p>What more can you do in the final hours of ...   \n",
       "4       <p>Say it loud, say it proud: Our rights, our ...   \n",
       "...                                                   ...   \n",
       "162319  <p>Rep. Katko voted for tax breaks for his wea...   \n",
       "162320  <p>Illinois early voting is open NOW &amp; you...   \n",
       "162321  <p>From your favorite peaks to the polling pla...   \n",
       "162322  <p>Beto just announced he’s running for presid...   \n",
       "162323  <p>Claim your FREE ACLU Voter sticker today to...   \n",
       "\n",
       "                                                thumbnail  \\\n",
       "0       https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
       "1       https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
       "2       https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
       "3       https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
       "4       https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
       "...                                                   ...   \n",
       "162319  https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
       "162320  https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
       "162321  https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
       "162322  https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
       "162323  https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
       "\n",
       "                           created_at                     updated_at   lang  \\\n",
       "0       2019-03-27 16:52:25.625455+00  2019-03-27 16:52:25.625455+00  en-US   \n",
       "1       2019-03-27 17:28:14.096849+00  2019-03-27 17:28:14.096849+00  en-US   \n",
       "2       2019-03-27 17:38:23.101377+00  2019-03-27 17:38:23.101377+00  en-US   \n",
       "3       2018-12-30 20:59:13.879124+00  2018-12-30 20:59:13.879124+00  en-US   \n",
       "4       2019-03-27 17:18:29.764002+00  2019-04-11 15:02:58.081112+00  en-US   \n",
       "...                               ...                            ...    ...   \n",
       "162319  2018-10-19 10:31:52.466563+00   2018-10-22 11:40:06.24382+00  en-US   \n",
       "162320  2018-10-24 20:41:42.111865+00  2018-10-24 20:41:42.111865+00  en-US   \n",
       "162321   2018-10-09 20:03:32.81012+00   2018-10-09 20:03:32.81012+00  en-US   \n",
       "162322  2019-03-15 03:07:40.590249+00   2019-03-22 17:01:05.36319+00  en-US   \n",
       "162323  2018-08-08 02:54:51.076959+00  2018-08-08 20:03:36.302904+00  en-US   \n",
       "\n",
       "        ... suppressed                                            targets  \\\n",
       "0       ...          f                                                 []   \n",
       "1       ...          f                                                 []   \n",
       "2       ...          f                                                 []   \n",
       "3       ...          f  [{\"target\": \"Activity on the Facebook Family\"}...   \n",
       "4       ...          f                                                 []   \n",
       "...     ...        ...                                                ...   \n",
       "162319  ...          f  [{\"target\": \"Age\", \"segment\": \"18 and older\"},...   \n",
       "162320  ...          f  [{\"target\": \"List\"}, {\"target\": \"Age\", \"segmen...   \n",
       "162321  ...          f  [{\"target\": \"Segment\", \"segment\": \"US politics...   \n",
       "162322  ...          f                                                 []   \n",
       "162323  ...          f  [{\"target\": \"Age\", \"segment\": \"35 to 54\"}, {\"t...   \n",
       "\n",
       "                                         advertiser  \\\n",
       "0                                               NaN   \n",
       "1                                               NaN   \n",
       "2                                               NaN   \n",
       "3                      Covenant House International   \n",
       "4                                               NaN   \n",
       "...                                             ...   \n",
       "162319                        Keep Them Accountable   \n",
       "162320  National Republican Congressional Committee   \n",
       "162321                              POW Action Fund   \n",
       "162322                                          NaN   \n",
       "162323                                         ACLU   \n",
       "\n",
       "                                                 entities  \\\n",
       "0       [{\"entity\": \"Endangered Species Act\", \"entity_...   \n",
       "1       [{\"entity\": \"Americans\", \"entity_type\": \"Group...   \n",
       "2       [{\"entity\": \"Zimbabwe\", \"entity_type\": \"Region\"}]   \n",
       "3                                                      []   \n",
       "4       [{\"entity\": \"Planned Parenthood\", \"entity_type...   \n",
       "...                                                   ...   \n",
       "162319     [{\"entity\": \"Katko\", \"entity_type\": \"Person\"}]   \n",
       "162320  [{\"entity\": \"Illinois\", \"entity_type\": \"Region...   \n",
       "162321                                                 []   \n",
       "162322      [{\"entity\": \"Beto\", \"entity_type\": \"Region\"}]   \n",
       "162323                                                 []   \n",
       "\n",
       "                                                     page  \\\n",
       "0                      https://www.facebook.com/LCVoters/   \n",
       "1              https://www.facebook.com/indivisibleguide/   \n",
       "2       https://www.facebook.com/InternationalRescueCo...   \n",
       "3                 https://www.facebook.com/CovenantHouse/   \n",
       "4             https://www.facebook.com/PlannedParenthood/   \n",
       "...                                                   ...   \n",
       "162319    https://www.facebook.com/KeepThemAccountable18/   \n",
       "162320                     https://www.facebook.com/NRCC/   \n",
       "162321            https://www.facebook.com/POWActionFund/   \n",
       "162322              https://www.facebook.com/betoorourke/   \n",
       "162323                     https://www.facebook.com/aclu/   \n",
       "\n",
       "                                               lower_page  \\\n",
       "0                      https://www.facebook.com/lcvoters/   \n",
       "1              https://www.facebook.com/indivisibleguide/   \n",
       "2       https://www.facebook.com/internationalrescueco...   \n",
       "3                 https://www.facebook.com/covenanthouse/   \n",
       "4             https://www.facebook.com/plannedparenthood/   \n",
       "...                                                   ...   \n",
       "162319    https://www.facebook.com/keepthemaccountable18/   \n",
       "162320                     https://www.facebook.com/nrcc/   \n",
       "162321            https://www.facebook.com/powactionfund/   \n",
       "162322              https://www.facebook.com/betoorourke/   \n",
       "162323                     https://www.facebook.com/aclu/   \n",
       "\n",
       "                                               targetings  \\\n",
       "0                                                     NaN   \n",
       "1                                                     NaN   \n",
       "2                                                     NaN   \n",
       "3       {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...   \n",
       "4                                                     NaN   \n",
       "...                                                   ...   \n",
       "162319  {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...   \n",
       "162320  {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...   \n",
       "162321  {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...   \n",
       "162322                                                NaN   \n",
       "162323  {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...   \n",
       "\n",
       "                                              paid_for_by targetedness  \\\n",
       "0                           League of Conservation Voters          NaN   \n",
       "1                                     Indivisible Project          NaN   \n",
       "2                          International Rescue Committee          NaN   \n",
       "3                            Covenant House International          5.0   \n",
       "4                Planned Parenthood Federation of America          NaN   \n",
       "...                                                   ...          ...   \n",
       "162319  HOUSE MAJORITY PAC, (202) 849-6052, AND PRIORI...          7.0   \n",
       "162320  the NRCC and not authorized by any candidate o...          4.0   \n",
       "162321                    Protect Our Winters Action Fund          4.0   \n",
       "162322                                   Beto for America          NaN   \n",
       "162323                                           the ACLU          6.0   \n",
       "\n",
       "       listbuilding_fundraising_proba  \n",
       "0                            0.647945  \n",
       "1                            0.350635  \n",
       "2                            0.999909  \n",
       "3                                 NaN  \n",
       "4                            0.999977  \n",
       "...                               ...  \n",
       "162319                       0.116965  \n",
       "162320                       0.312412  \n",
       "162321                       0.205220  \n",
       "162322                       0.999994  \n",
       "162323                       0.354132  \n",
       "\n",
       "[162324 rows x 24 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb = pd.read_csv('fbpac-ads-en-US.csv.xz', dtype={'message': 'string', 'title': 'string', 'paid_for_by':'string'}) #'/Users/mehrunisaqayyum/Downloads/work/fbpac.csv'\n",
    "fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'html', 'political', 'not_political', 'title', 'message',\n",
       "       'thumbnail', 'created_at', 'updated_at', 'lang', 'images',\n",
       "       'impressions', 'political_probability', 'targeting', 'suppressed',\n",
       "       'targets', 'advertiser', 'entities', 'page', 'lower_page', 'targetings',\n",
       "       'paid_for_by', 'targetedness', 'listbuilding_fundraising_proba'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                 object\n",
       "html                               object\n",
       "political                           int64\n",
       "not_political                       int64\n",
       "title                              string\n",
       "message                            string\n",
       "thumbnail                          object\n",
       "created_at                         object\n",
       "updated_at                         object\n",
       "lang                               object\n",
       "images                             object\n",
       "impressions                         int64\n",
       "political_probability             float64\n",
       "targeting                          object\n",
       "suppressed                         object\n",
       "targets                            object\n",
       "advertiser                         object\n",
       "entities                           object\n",
       "page                               object\n",
       "lower_page                         object\n",
       "targetings                         object\n",
       "paid_for_by                        string\n",
       "targetedness                      float64\n",
       "listbuilding_fundraising_proba    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>message</th>\n",
       "      <th>paid_for_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>League of Conservation Voters</td>\n",
       "      <td>&lt;p&gt;BREAKING: Trump’s Department of the Interio...</td>\n",
       "      <td>League of Conservation Voters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Indivisible Guide</td>\n",
       "      <td>&lt;p&gt;The Mueller investigation is over. Special ...</td>\n",
       "      <td>Indivisible Project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>International Rescue Committee</td>\n",
       "      <td>&lt;p&gt;Zimbabwe is reeling from the impact of Cycl...</td>\n",
       "      <td>International Rescue Committee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Covenant House International</td>\n",
       "      <td>&lt;p&gt;What more can you do in the final hours of ...</td>\n",
       "      <td>Covenant House International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Planned Parenthood</td>\n",
       "      <td>&lt;p&gt;Say it loud, say it proud: Our rights, our ...</td>\n",
       "      <td>Planned Parenthood Federation of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162309</th>\n",
       "      <td>AFT - American Federation of Teachers</td>\n",
       "      <td>&lt;p&gt;Mike DeWine was elected as Ohio’s attorney ...</td>\n",
       "      <td>AFT - American Federation of Teachers, not aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162310</th>\n",
       "      <td>MoveOn</td>\n",
       "      <td>&lt;p&gt;We resisted and won control of the House! N...</td>\n",
       "      <td>MoveOn.org Political Action, pol.moveon.org, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162311</th>\n",
       "      <td>AAAS - The American Association for the Advanc...</td>\n",
       "      <td>&lt;p&gt;Diversity in science, technology, and engin...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162312</th>\n",
       "      <td>Nathan Fletcher for County Supervisor 2018</td>\n",
       "      <td>&lt;p&gt;As Supervisor, I will stand up to the polit...</td>\n",
       "      <td>Nathan Fletcher for Supervisor 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162313</th>\n",
       "      <td>Industry City</td>\n",
       "      <td>&lt;p&gt;Support Industry City's plan to create 15,0...</td>\n",
       "      <td>Industry City</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162314 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "0                           League of Conservation Voters   \n",
       "1                                       Indivisible Guide   \n",
       "2                          International Rescue Committee   \n",
       "3                            Covenant House International   \n",
       "4                                      Planned Parenthood   \n",
       "...                                                   ...   \n",
       "162309              AFT - American Federation of Teachers   \n",
       "162310                                             MoveOn   \n",
       "162311  AAAS - The American Association for the Advanc...   \n",
       "162312         Nathan Fletcher for County Supervisor 2018   \n",
       "162313                                      Industry City   \n",
       "\n",
       "                                                  message  \\\n",
       "0       <p>BREAKING: Trump’s Department of the Interio...   \n",
       "1       <p>The Mueller investigation is over. Special ...   \n",
       "2       <p>Zimbabwe is reeling from the impact of Cycl...   \n",
       "3       <p>What more can you do in the final hours of ...   \n",
       "4       <p>Say it loud, say it proud: Our rights, our ...   \n",
       "...                                                   ...   \n",
       "162309  <p>Mike DeWine was elected as Ohio’s attorney ...   \n",
       "162310  <p>We resisted and won control of the House! N...   \n",
       "162311  <p>Diversity in science, technology, and engin...   \n",
       "162312  <p>As Supervisor, I will stand up to the polit...   \n",
       "162313  <p>Support Industry City's plan to create 15,0...   \n",
       "\n",
       "                                              paid_for_by  \n",
       "0                           League of Conservation Voters  \n",
       "1                                     Indivisible Project  \n",
       "2                          International Rescue Committee  \n",
       "3                            Covenant House International  \n",
       "4                Planned Parenthood Federation of America  \n",
       "...                                                   ...  \n",
       "162309  AFT - American Federation of Teachers, not aut...  \n",
       "162310  MoveOn.org Political Action, pol.moveon.org, n...  \n",
       "162311                                               <NA>  \n",
       "162312                Nathan Fletcher for Supervisor 2018  \n",
       "162313                                      Industry City  \n",
       "\n",
       "[162314 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe = ['title','message','paid_for_by']\n",
    "text_fb = fb[fe]\n",
    "text_fb.head(-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning\n",
    "We will review text in columns 'title','message','paid_for_by' for our Natural Language Processing project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Remove punctuation and \"weird stuff like --\" from ['title','message','paid_for_by'].'''\n",
    "\n",
    "import re\n",
    "\n",
    "def text_cleaner(text_fb):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text_fb)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text_fb)\n",
    "    text = ' '.join(text_fb.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.4'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          string\n",
       "message        string\n",
       "paid_for_by    string\n",
       "dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need values to be strings \n",
    "text_fb.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    return text\n",
    "\n",
    "# <.*?>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here is some text for you to clean sorry about the caps loko'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_round1(\"HERE IS SOME text for you to clean, sorry about the CAPS LOKO.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'breaking trump’s department of the interior plans to remove endangered species act protections for gray wolves without these safeguards gray wolf populations could decline across the country we must fight back on this attack on our wildlife'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_round1(text_fb['message'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BREAKING: Trump’s Department of the Interior plans to remove Endangered Species Act protections for gray wolves. Without these safeguards, gray wolf populations could decline across the country. We must fight back on this attack on our wildlife.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('<.*?>', '', text_fb['message'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "text_fb['message_round1'] = text_fb['message'].apply(clean_text_round1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "text_fb['title_round1'] = text_fb['title'].apply(clean_text_round1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       league of conservation voters\n",
       "1                                   indivisible guide\n",
       "2                      international rescue committee\n",
       "3                        covenant house international\n",
       "4                                  planned parenthood\n",
       "                             ...                     \n",
       "162319                          keep them accountable\n",
       "162320    national republican congressional committee\n",
       "162321                                pow action fund\n",
       "162322                                  beto o'rourke\n",
       "162323                                           aclu\n",
       "Name: title, Length: 162324, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_fb.title.str.lower()\n",
    "#applied lower case method to title column\n",
    "#how to iterate over every row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7bb4c95957e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Example: alice = text_cleaner(alice)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext_fb_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text_round1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_fb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-00d68bccf029>\u001b[0m in \u001b[0;36mclean_text_round1\u001b[0;34m(text_fb)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_fb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaid_for_by\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#text = text_fb['message']=dataframe['message'].apply(str)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\[.*?\\]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_fb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[%s]'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_fb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\w*\\d\\w*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_fb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "#Example: alice = text_cleaner(alice)\n",
    "text_fb_nlp = clean_text_round1(text_fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methodology Note: \n",
    "We need to parse the cleaned data. The cleaned data represents the Facebook text in the 'message' column. We could include the 'title' column as well to match the 'title' with the corresponding message as a measure of checking authenticity the way the New York Times Challenge demonstrated.\n",
    "\n",
    "Should we undertake: Topic Modeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned ads. This can take a bit.\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "#fb_ad_doc = nlp(text_fb_nlp)  #doc = nlp(\"string\")\n",
    "fb_ad_doc = nlp(text)\n",
    "\n",
    "#Iterate over tokens in a doc\n",
    "#for token in fb_ad_doc:\n",
    " #   print(token.text) #print (token.text)\n",
    "    \n",
    "    #Don't forget sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing second Facebook dataset to argue for business case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_likes = pd.read_csv('/Users/mehrunisaqayyum/Downloads/pseudo_facebook.csv')\n",
    "fb_likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating\n",
    "fb_df2 = pd.concat([fb, fb_likes], ignore_index=True, sort =True)\n",
    "fb_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Description: Second Facebook Dataset\n",
    "\n",
    "ad_id is the id of specific ad set | Numeric\n",
    "\n",
    "Reporting_start and reporting_end are the start and end dates of the each ad | Numeric\n",
    "\n",
    "Campaign_id is the id assigned by the ad running company | Numeric- Negligible\n",
    "\n",
    "fb_campaign_id is the id assigned by facebook for every ad set| Numeric- Negligible\n",
    "\n",
    "age and gender talk about the demographics | Categorical\n",
    "\n",
    "Interest1, Interest2, Interest3 are the user interests and likes of facebook users who were targeted for the ad | Categorical \n",
    "\n",
    "Impressions are the number of times the ad was shown to the users |Numeric\n",
    "\n",
    "Clicks is the number of time users clicked on the ad | Numeric\n",
    "\n",
    "spent is the amount of money spent on each campaign | Numeric\n",
    "\n",
    "Totalconversions is the number of users who have clicked the ad and have made a purchase or installed an app\n",
    "approved_conversions tells how many became actual active users | Numerica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation: We have columns like \"political\", 'not_political', 'title' and 'message' of the advertisement; 'created at'; 'lang' for languages; 'political_probabilty', and 'paid_for_by'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics: Average Dollars Spent on Facebook Ads\n",
    "From another dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df = pd.read_csv('/Users/mehrunisaqayyum/Downloads/datasets_104115_247225_data.csv')\n",
    "clicks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used 'clicks' and 'spent' to calculate the cost per click and per ad.\n",
    "\n",
    "click_count_per_add = clicks_df.clicks.sum()/clicks_df.clicks.count()\n",
    "click_ratio = (clicks_df.spent.sum()/63)/clicks_df.clicks.count()\n",
    "ad_spend_count_ratio = (clicks_df.spent.sum()/63)/clicks_df.clicks.count()\n",
    "print('Total number of Ads purcheased by IRA from 2015 to 2017: ', clicks_df.clicks.count())\n",
    "print('Total dollar spent by IRA from 2015 to 2017: ${:.2f}'.format(clicks_df.spent.sum()/63))\n",
    "print('Average cost per Ad: ${:.2f}'.format(ad_spend_count_ratio))\n",
    "print('Average number of clicks per Ad: {:.2f}'.format(click_count_per_add))\n",
    "print('Avg cost per click: ${:.2f}'.format(click_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "We see that we have 162,314 records of political ad data on Facebook. The last ten sample how ads were purchased by nonpartisan groups, like \"League of Conservation Voters\", political organizer groups, like \"Indivisible Project\", unions \"AFT\", and international nonprofits like \"International Rescue Committee\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle Data on Facebook ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "data_df.to_pickle(\"corpus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=set(stopwords.words('english'))\n",
    "\n",
    "def build_list(fb,col=\"title\"):\n",
    "    corpus=[]\n",
    "    lem=WordNetLemmatizer()\n",
    "    stop=set(stopwords.words('english'))\n",
    "    new= fb[col].dropna().str.split()\n",
    "    new=new.values.tolist()\n",
    "    corpus=[lem.lemmatize(word.lower()) for i in new for word in i if(word) not in stop]\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Analysis: Turning FB message into Vectors\n",
    "We note the stop words and review counts of words from tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning FB message into Vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(emma_paras, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=False, #don't convert everything to lower case (since proper names are people who are targeted in disinfo campaigns)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "fb_message_tfidf =vectorizer.fit_transform(text_fb)\n",
    "print(\"Number of features: %d\" % fb_message_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(fb_message_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "print(terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=build_list(text_fb)\n",
    "counter=Counter(corpus)\n",
    "most=counter.most_common()\n",
    "x=[]\n",
    "y=[]\n",
    "for word,count in most[:20]:\n",
    "    if (word not in stop) :\n",
    "        x.append(word)\n",
    "        y.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "sns.barplot(x=y,y=x)\n",
    "plt.title(\"most common word in title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "Top 5 most common words are ranked:\n",
    "    1) 'Committee'\n",
    "    2)'International'\n",
    "    3) 'Action' and 'Planned'\n",
    "    4) 'Parenthood'\n",
    "    \n",
    "USA and 'America' are interchangeable--so maybe these are double-counting. \n",
    "\n",
    "'Democratic' not used as much as nonpolitical word of 'rescue'. However, we do not see 'Republican' or 'GOP'. \n",
    "\n",
    "'Beto' and'O'Rourke' are occuring at same rate. This is the only person to show up in this political ad data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common 'Paid For By'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=build_list(text_fb,\"paid_for_by\")\n",
    "counter=Counter(corpus)\n",
    "most=counter.most_common()\n",
    "x=[]\n",
    "y=[]\n",
    "for word,count in most[:10]:\n",
    "    if (word not in stop) :\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "        \n",
    "plt.figure(figsize=(9,7))\n",
    "sns.barplot(x=y,y=x)\n",
    "plt.title(\"Most Common Word in 'paid_for_by'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure of 'Number of Top Ad Titles'\n",
    "To highlight the ads that we would should review for fake versus true classifier to strenghten business case for impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count(feature, title,fb, size=1, show_percents=False):\n",
    "    f, ax = plt.subplots(1,1, figsize=(4*size, 4))\n",
    "    total = float(len(fb))\n",
    "    g = sns.countplot(fb[feature],order = fb[feature].value_counts().index[0:20], palette='Set3')\n",
    "    g.set_title(\"Number of {}\".format(title))\n",
    "    if (size > 2):\n",
    "        plt.xticks(rotation=90, size=10)\n",
    "    if(show_percents):\n",
    "        for p in ax.patches:\n",
    "            height = p.get_height()\n",
    "            ax.text(p.get_x() + p.get_width()/2.,\n",
    "                   height + 3, '{:1.2f}%'.format(100*height/total),\n",
    "                   ha=\"center\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels());\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count('title','Top Ad Titles', text_fb, 3.5)\n",
    "#plt.title(\"Number of Top Ad Titles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Number of Most Popular Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note the number (counts) of message.\n",
    "plot_count('message','message countplot', text_fb, 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "Defining functions to identify most common words and the create features from those words in the text. We need to create a df of the \"corpus\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to use fb_text_df.\n",
    " ##Do we need to create features from Bag of Words and vectorize them for the Random Forest Classifier?\n",
    "def bag_of_words(text):\n",
    "    allwords = [token.lemma_\n",
    "               for token in text\n",
    "               if not token.is_punct\n",
    "               and not token.is_stop]\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_features(sentences, common_words):\n",
    "    fb_text_df = pd.DataFrame(columns=common_words)\n",
    "    fb_text_df['text_sentence'] = sentences[0]\n",
    "    fb_text_df['text_source'] = sentences[1]\n",
    "    fb_text_df.loc[:,common_words] = 0\n",
    "    \n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        words = [token.lemma_ \n",
    "                for token in sentence\n",
    "                if (\n",
    "                    not token.is_punct\n",
    "                    and not token.is_stop\n",
    "                    and token.lemma_ in common_words\n",
    "                )]\n",
    "        for word in words:\n",
    "            fb_text_df.loc[i, word] += 1\n",
    "        if i%100 == 0:\n",
    "            print('Processing row {}'.format(i))\n",
    "    return fb_text_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data means fb_text_df\n",
    "top_words_in_ads_dict = {}\n",
    "for c in fb_text_df.columns:\n",
    "    top = fb_text_df[c].sort_values(ascending=False).head(30)\n",
    "    top_words_in_ads_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "top_words_in_ads_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['message'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.message.iloc[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_variable = \"\".join(fb.message.iloc[1:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sentences into numeric vectors.\n",
    "We need to transform sentences into NUMERIC vectors so that the vectors can be included in a Random Forest Classifier model, which cannot use string values.\n",
    "\n",
    "First we must create list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List \n",
    "\n",
    "message_propoganda_list = []\n",
    "for topic in [#Need to include topic_raw like physics_raw]:\n",
    "    for sentence in topic['title','paid_for_by']['docs']: #What is docs?\n",
    "        message_propoganda_list = message_propoganda_list + sentence['message']  #Used message_propaganda for abstract\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use spacy to count frequency of words\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# All the processing work is done here, so it may take a while.\n",
    "FB_political_doc = nlp(temp_variable)\n",
    "#FB_popuarity_doc = nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned Text before Identifying 'Stopwords'\n",
    "from collections import Counter\n",
    "\n",
    "# Utility function to calculate how frequently words appear in the data sets.\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of words.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(words)\n",
    "    \n",
    "# The most frequent words:\n",
    "political_freq = word_frequencies(temp_variable).most_common(10)\n",
    "#like_freq = word_frequencies().most_common(10)\n",
    "print('Political:', political_freq)\n",
    "#print('Popular:', like_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "Source: https://github.com/adashofdata/nlp-in-python-tutorial/blob/master/3-Sentiment-Analysis.ipynb Regarding text data, there are a few popular techniques that we'll be going through in the next few notebooks, starting with sentiment analysis. A few key points to note with sentiment analysis.\n",
    "\n",
    "TextBlob Module: Linguistic researchers have labeled the sentiment of words based on their domain expertise. Sentiment of words can vary based on where it is in a sentence. The TextBlob module allows us to take advantage of these labels. Sentiment Labels: Each word in a corpus is labeled in terms of polarity and subjectivity (there are more labels as well, but we're going to ignore them for now). A corpus' sentiment is the average of these. Polarity: How positive or negative a word is. -1 is very negative. +1 is very positive. Subjectivity: How subjective, or opinionated a word is. 0 is fact. +1 is very much an opinion. For more info on how TextBlob coded up its sentiment function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_df2.loc[0:5, 'message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob('The Mueller investigation is over').sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create quick lambda functions to find the polarity and subjectivity of each message\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge textblob\n",
    "\n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "fb_df2['polarity'] = fb_df2['message'].apply(pol)\n",
    "fb_df2['subjectivity'] = fb_df2['title'].apply(sub)\n",
    "fb_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop each 'message' value\n",
    "\n",
    "for token in text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
