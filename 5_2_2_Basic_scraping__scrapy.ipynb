{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "12px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "5.2.2 Basic scraping with scrapy.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pitaconsumer/Jupyter-Projects/blob/master/5_2_2_Basic_scraping__scrapy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "id": "j50Duq3j-zJj",
        "colab_type": "text"
      },
      "source": [
        "## Scrapy\n",
        "\n",
        "Unlike the other Python packages we have used so far, scrapy comes with its own API. While we haven't covered API's just yet, what that means practically is that scrapy makes it easy to write and debug spiders (the little programs that crawl/scrape the website).  You can also run scrapy in the command line or jupyter notebook.  For ease of illustration, the examples we use will be in the jupyter notebook.  If you want to pursue scraping further, we enourage you to use the [API](https://doc.scrapy.org/en/latest/topics/api.html). You can also see an example of a project that uses that API [here](https://doc.scrapy.org/en/latest/intro/examples.html)\n",
        "\n",
        "Here we will write a scraper to pull data from the [_Everyday Sexism Project_](https://everydaysexism.com/), which has been collecting stories about times people have experienced or observed sexism since 2012. User-provided entries are stored and displayed as simple text entries, with user-provided categories. There is no API or other apparent method to access the data aside from the webpage, and experiences of sexism are so subjective that it is difficult to imagine getting similar-quality information through a questionnaire or other easy-to-analyse data collection method. Yet as presented, it isn’t possible to see any overarching trends beyond the sheer quantity of entries.\n",
        "\n",
        "A data scientist with a specific interest in learning more about experiences of sexism might want to write a scraper for the _Everyday Sexism Project_. A good scraper will pull data from every page of the ESP website. We want to save the entry text, any category labels, date posted, and the name provided by the poster. Our scraper will scrape each page, then find the URL for the previous page and call itself again for that page.\n",
        "\n",
        "\n",
        "## Basic Scraper\n",
        "\n",
        "Let's start with a simple scraper that just saves the entire webpage as an HTML file.  Note that Scrapy is built on top of the [Twisted](http://twistedmatrix.com/trac/) [reactor](http://stackoverflow.com/questions/35111265/how-does-pythons-twisted-reactor-work) which is event-driven and cannot be restarted within a session. What this means for us is that after each time you run a Scrapy script example here, you'll need to restart the kernel before you run another Scrapy script. Just another reason to use scrapy in the command line...\n",
        "\n",
        "In a Scrapy-made scraper, you provide one or more starting URLs. The scraper then sends a request to the server for the information at that URL. The server provides a response, which is either the information requested or an error code. Scrapy then processes the response in accordance with the instructions you've given it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEw07MLMZzvA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ac63ac58-16fe-48db-db9d-abde7a9bfcd0"
      },
      "source": [
        "!pip install scrapy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scrapy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/b1/105fe9a289e5bb64ec104076546f72060296d9989a0fc31a8b608c810868/Scrapy-2.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 9.0MB/s \n",
            "\u001b[?25hCollecting zope.interface>=4.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/33/565274c28a11af60b7cfc0519d46bde4125fcd7d32ebc0a81b480d0e8da6/zope.interface-5.1.0-cp36-cp36m-manylinux2010_x86_64.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 39.3MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=16.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (4.2.6)\n",
            "Collecting PyDispatcher>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
            "Collecting w3lib>=1.17.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Collecting protego>=0.1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/6e/bf6d5e4d7cf233b785719aaec2c38f027b9c2ed980a0015ec1a1cced4893/Protego-0.1.16.tar.gz (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 36.8MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.1\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting itemadapter>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/fb/92f848fcfa85dc9f95370eaecb5c99b5230dd4fc5c6bae684f4ca59df973/itemadapter-0.1.0-py3-none-any.whl\n",
            "Collecting parsel>=1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/23/1e/9b39d64cbab79d4362cdd7be7f5e9623d45c4a53b3f7522cd8210df52d8e/parsel-1.6.0-py2.py3-none-any.whl\n",
            "Collecting service-identity>=16.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
            "Collecting queuelib>=1.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
            "Collecting Twisted>=17.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/04/1a664c9e5ec0224a1c1a154ddecaa4dc7b8967521bba225efcc41a03d5f3/Twisted-20.3.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 48.4MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/04/686efee2dcdd25aecf357992e7d9362f443eb182ecd623f882bc9f7a6bba/cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 51.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.1.3->scrapy) (49.1.0)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from pyOpenSSL>=16.2.0->scrapy) (1.12.0)\n",
            "Requirement already satisfied: attrs>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (19.3.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n",
            "Collecting constantly>=15.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
            "Collecting Automat>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/dd/83/5f6f3c1a562674d65efc320257bdc0873ec53147835aeef7762fe7585273/Automat-20.2.0-py2.py3-none-any.whl\n",
            "Collecting incremental>=16.10.1\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
            "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/16/e54cc65891f01cb62893540f44ffd3e8dab0a22443e1b438f1a9f5574bee/PyHamcrest-2.0.2-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n",
            "Building wheels for collected packages: PyDispatcher, protego\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp36-none-any.whl size=11515 sha256=98d73cd4216965e59158a990796b5580bfb19c1088975ce7bc9bfa297f42a105\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
            "  Building wheel for protego (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for protego: filename=Protego-0.1.16-cp36-none-any.whl size=7765 sha256=df571afeb8651e09a2cba02f1caa24e98ddc8527419ffdd63d7725b46a040272\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/01/d1/4a2286a976dccd025ba679acacfe37320540df0f2283ecab12\n",
            "Successfully built PyDispatcher protego\n",
            "Installing collected packages: zope.interface, cryptography, pyOpenSSL, PyDispatcher, w3lib, protego, cssselect, itemadapter, parsel, service-identity, queuelib, hyperlink, constantly, Automat, incremental, PyHamcrest, Twisted, scrapy\n",
            "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Twisted-20.3.0 constantly-15.1.0 cryptography-2.9.2 cssselect-1.1.0 hyperlink-19.0.0 incremental-17.5.0 itemadapter-0.1.0 parsel-1.6.0 protego-0.1.16 pyOpenSSL-19.1.0 queuelib-1.5.0 scrapy-2.2.0 service-identity-18.1.0 w3lib-1.22.0 zope.interface-5.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU6MKXru-zJl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d892a9ea-aba7-4e81-edd0-4e23d3610fa9"
      },
      "source": [
        "# Importing in each cell because of the kernel restarts.\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "\n",
        "class ESSpider(scrapy.Spider):\n",
        "    # Naming the spider is important if you are running more than one spider of\n",
        "    # this class simultaneously.\n",
        "    name = \"ESS\"\n",
        "    \n",
        "    # URL(s) to start with.\n",
        "    start_urls = [\n",
        "        'http://www.everydaysexism.com',\n",
        "    ]\n",
        "\n",
        "    # What to do with the URL.  Here, we tell it to download all the code and save\n",
        "    # it to the mainpage.html file\n",
        "    def parse(self, response):\n",
        "        with open('mainpage.html', 'wb') as f:\n",
        "            f.write(response.body)\n",
        "\n",
        "\n",
        "# Instantiate our crawler.\n",
        "process = CrawlerProcess()\n",
        "\n",
        "# Start the crawler with our spider.\n",
        "process.crawl(ESSpider)\n",
        "process.start()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-24 03:38:34 [scrapy.utils.log] INFO: Scrapy 2.1.0 started (bot: scrapybot)\n",
            "2020-06-24 03:38:34 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Apr 18 2020, 01:56:04) - [GCC 8.4.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Linux-4.19.104+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2020-06-24 03:38:34 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2020-06-24 03:38:34 [scrapy.crawler] INFO: Overridden settings:\n",
            "{}\n",
            "2020-06-24 03:38:34 [scrapy.extensions.telnet] INFO: Telnet Password: 640c77a168f818ff\n",
            "2020-06-24 03:38:34 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2020-06-24 03:38:34 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2020-06-24 03:38:34 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2020-06-24 03:38:34 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2020-06-24 03:38:34 [scrapy.core.engine] INFO: Spider opened\n",
            "2020-06-24 03:38:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2020-06-24 03:38:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2020-06-24 03:38:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.everydaysexism.com/> from <GET http://www.everydaysexism.com>\n",
            "2020-06-24 03:38:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://everydaysexism.com/> from <GET https://www.everydaysexism.com/>\n",
            "2020-06-24 03:38:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://everydaysexism.com/> (referer: None)\n",
            "2020-06-24 03:38:36 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2020-06-24 03:38:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 662,\n",
            " 'downloader/request_count': 3,\n",
            " 'downloader/request_method_count/GET': 3,\n",
            " 'downloader/response_bytes': 16074,\n",
            " 'downloader/response_count': 3,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'downloader/response_status_count/301': 2,\n",
            " 'elapsed_time_seconds': 1.533179,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2020, 6, 24, 3, 38, 36, 306429),\n",
            " 'log_count/DEBUG': 3,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 182452224,\n",
            " 'memusage/startup': 182452224,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 3,\n",
            " 'scheduler/dequeued/memory': 3,\n",
            " 'scheduler/enqueued': 3,\n",
            " 'scheduler/enqueued/memory': 3,\n",
            " 'start_time': datetime.datetime(2020, 6, 24, 3, 38, 34, 773250)}\n",
            "2020-06-24 03:38:36 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46HI0E8W-zJw",
        "colab_type": "text"
      },
      "source": [
        "If that worked, you should now have a file called 'mainpage.html' saved to your machine that contains all the code from www.everydaysexism.com.  Talk to your mentor if you are having problems with this step, and make sure it is working before you move forward.\n",
        "\n",
        "By default Scrapy gives detailed logging, which you should see above in red.\n",
        "\n",
        "Oh, and remember, you'll need to restart your kernel if you want to rerun a Scrapy script. If you get an error, the first thing to try is restarting your kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTJVKM8i-zJx",
        "colab_type": "text"
      },
      "source": [
        "## Parsing HTML\n",
        "\n",
        "Now that we know the spider can pull the code, we need to _do_ something with it.\n",
        "\n",
        "Scraping the web means learning a bit about web technologies: HTML, XML, and CSS. This code contains the content and display information for every web page. You'll need to get familiar with the structure of your document and the code that identifies the piece(s) of the web page you want. To see the code for an entry on www.everydaysexism.com, open the page in the web browser of your choice and hover over the brownish background rectangle that defines an entry.  Left-click, then select \"Inspect Element\" or some variant- [the terminology will vary depending on your browser](https://victorfont.com/how-to-use-your-browsers-inspect-tool/). In Chrome, the command is \"Inspect\". This will display your browser's developer tools alongside or below the rendered web page. The \"Elements\" tab of your developer tools will show the code for the page. The particular element you left-clicked on will be highlighted. As you move around in the code, different parts of the webpage will be highlighted, letting you know what that piece of code refers to.\n",
        "\n",
        "![Inspecting web page elements](assets/developer_tools.gif)\n",
        "\n",
        "If you've done it right, your click should have highlighted code starting with `<article id=\"post-`.  Each submission on the page is contained within an `<article>` element. Nested within each article is a `<header>` element, a `<section>` element (with the class `\"entry-content\"`), and a `<footer>` element.  Look and click around a bit and see if you can find the information we want to extract from each submission: (1) The author, (2) the date it was posted, (3) the text content, and (4) any tags associated with it. We're lucky that every submission has a standardized format, so we can use the code for this one submission as a template to build our scraper's parsing instructions.\n",
        "\n",
        "So how do we get the information out of the HTML code and into our parser?  Often, people just learning to scrape think \"a ha! I will use regular expressions to pick out what I want!\"  This [is not a good idea](https://blog.codinghorror.com/parsing-html-the-cthulhu-way/) in most cases. HTML is a context-based language – the content described by any given HTML tag depends on where that tag is positioned relative to all other tags, and where that tag is opened and closed. Regex, on the other hand, ignores context. It goes along until it finds the conditions you've described and grabs the material there, ignoring everything else.  Combining HTML and regex creates [fragile code that breaks easily](http://stackoverflow.com/questions/6751105/why-its-not-possible-to-use-regex-to-parse-html-xml-a-formal-explanation-in-la).\n",
        "\n",
        "Fortunately, there are context-aware parsers for HTML and XML that allow you to find what you need within the code without a headache.  Scrapy can work with any parser you provide, and natively incorporates two \"selectors\": [CSS-based and Xpath](https://doc.scrapy.org/en/latest/topics/selectors.html). We'll be using [Xpath](https://www.w3schools.com/xml/xpath_intro.asp) here: It treats nested code like computer file paths, with each level referred to as a 'node' (eg. C:\\\\User\\Desktop\\Stuff for files, \\\\body\\header\\p for code).\n",
        "\n",
        "The Xpath expression to find the title of a submission in our page is: `//article/header/h2/a/@title`.  In English, this means:  \n",
        "\n",
        " * `//article`: _Locate every_ `article` _node in the page._\n",
        " * `/header`: _Find the_ `header` _node nested directly underneath the_ `article` _node._  \n",
        " * `/h2`: _Find the_ `h2` _node nested directly underneath the_ `header` _node_.  \n",
        " * `/a`: _Find the_ `a` _node nested directly underneath the_ `h2` _node._  \n",
        " * `/@title`: _Find the_ `title` _attribute within the_ `a` _node._  \n",
        "\n",
        "The best way to learn Xpath is to try it out.  You can easily experiment using [this XPath tester](http://www.freeformatter.com/xpath-tester.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb1PBoNS-zJx",
        "colab_type": "text"
      },
      "source": [
        "## Using JSON to store output\n",
        "\n",
        "As we saw in the previous example, Scrapy writes information to a file.  But raw HTML isn't very useful. Here, we will use the [JSON file format](https://blog.datafiniti.co/4-reasons-you-should-use-json-instead-of-csv-2cac362f1943). JSON stores data in a format that looks a lot like Python dictionaries. This is nice for scraping where you are likely to get non-flat data (also called semi-structured data), where rows can have different numbers of columns. JSON files are also easy to read back into Python.\n",
        "\n",
        "Because we're running Scrapy in a script, we need to call an instance of the CrawlerProcess class to actually run our scrapers. (In the API, this is taken care of automatically.) We can pass arguments into the class call that will give instructions to the scrapers, such as where to store the data (in this case, a JSON file called `firstpage.json`).\n",
        "\n",
        "Remember, scrapy can't restart itself, so you have to restart the kernel before running this code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "scrolled": true,
        "id": "YAYz53h3-zJy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "aec8043f-d158-4806-e945-205d23d86dad"
      },
      "source": [
        "# Importing in each cell because of the kernel restarts.\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "\n",
        "class ESSpider(scrapy.Spider):\n",
        "    # Naming the spider is important if you are running more than one spider of\n",
        "    # this class simultaneously.\n",
        "    name = \"ESS\"\n",
        "    \n",
        "    # URL(s) to start with.\n",
        "    start_urls = [\n",
        "        'http://www.everydaysexism.com',\n",
        "    ]\n",
        "\n",
        "    \n",
        "\n",
        "    # Use XPath to parse the response we get.\n",
        "    def parse(self, response):\n",
        "        \n",
        "        # Iterate over every <article> element on the page.\n",
        "        for article in response.xpath('//article'):\n",
        "            \n",
        "            # Yield a dictionary with the values we want.\n",
        "            yield {\n",
        "                # This is the code to choose what we want to extract\n",
        "                # You can modify this with other Xpath expressions to extract other information from the site\n",
        "                'name': article.xpath('header/h2/a/@title').extract_first(),\n",
        "                'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
        "                'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
        "                'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
        "            }\n",
        "\n",
        "# Tell the script how to run the crawler by passing in settings.\n",
        "process = CrawlerProcess({\n",
        "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
        "    'FEED_URI': 'firstpage.json',  # Name our storage file.\n",
        "    'LOG_ENABLED': False           # Turn off logging for now.\n",
        "})\n",
        "\n",
        "# Start the crawler with our spider.\n",
        "process.crawl(ESSpider)\n",
        "process.start()\n",
        "print('Success!')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scrapy/extensions/feedexport.py:210: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
            "  exporter = cls(crawler)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgQ_YEg9-zJ3",
        "colab_type": "text"
      },
      "source": [
        "# Checking the output\n",
        "\n",
        "Did it work? We turned off logging in the example above, so the best way to check is to look at our output JSON file: `firstpage.json`.\n",
        "\n",
        "**Note**: if you run the code more than once in a row, Scrapy will try to append the new scraped information to the end of the old JSON file. This will cause the JSON file to bug out because it will have an \"end of file\" notation in the middle of the file, so make sure to delete your JSON file before you re-run this or you'll get a \"trailing file\" error.\n",
        "\n",
        "We can take a look at the raw json file (which is pretty human-readable) in your favorite text editor, or load it into Python as a data frame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8iXLI3E-zJ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "3691b31d-daa9-47cd-9759-161d62763d25"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "firstpage = pd.read_json('firstpage.json', orient='records')\n",
        "print(firstpage.shape)\n",
        "print(firstpage.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 4)\n",
            "        name  ...                             tags\n",
            "0  anonymous  ...                         [School]\n",
            "1    Georgia  ...                               []\n",
            "2         Em  ...  [Always on alert, Public space]\n",
            "3       Anon  ...                         [School]\n",
            "4          J  ...           [Public space, School]\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mWmnqk6-zJ_",
        "colab_type": "text"
      },
      "source": [
        "Success!  Ten rows (representing ten entries) each with date, name, tags, and text.  \n",
        "\n",
        "\n",
        "## Recursion\n",
        "\n",
        "Now that we have a scraper that can pull the information we want off of a page and store it in a file, we want to run that scraper over _all_ the pages of the website. We do this using recursion – the Scrapy spider will run over a page, gather information, and then detect a link to the next page and _call itself_ on the new page.  Then the new instance of the spider will do the same thing. This will repeat until it arrives at a page with no link to a next page, or when it reaches a stopping condition we define.\n",
        "\n",
        "Since _Everyday Sexism_ has a paginated structure (every page links to the page before it), we can use recursion to get all the entries on the site. If you wanted to scrape a website with a non-linear structure, you'd still use the recursive setup, but with different restrictions on what links the scraper should follow. Scrapy is efficient and will only scrape each page once, even if it travels to them multiple times.\n",
        "\n",
        "You may notice we've added some additional arguments to the ´CrawlerProcess()´ command- these have to do with scraping etiquette and will be introduced in the next assignment.\n",
        "\n",
        "Again, you'll want to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "eK7sx4FE-zKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "f26cbec0-9f9b-426f-8617-34e3c815432a"
      },
      "source": [
        "# Importing in each cell because of the kernel restarts.\n",
        "import scrapy\n",
        "import re\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "class ESSpider(scrapy.Spider):\n",
        "    # Naming the spider is important if you are running more than one spider of\n",
        "    # this class simultaneously.\n",
        "    name = \"ESS\"\n",
        "    \n",
        "    # URL(s) to start with.\n",
        "    start_urls = [\n",
        "        'http://www.everydaysexism.com',\n",
        "    ]\n",
        "\n",
        "    # Use XPath to parse the response we get.\n",
        "    def parse(self, response):\n",
        "        \n",
        "        # Iterate over every <article> element on the page.\n",
        "        for article in response.xpath('//article'):\n",
        "            \n",
        "            # Yield a dictionary with the values we want.\n",
        "            yield {\n",
        "                'name': article.xpath('header/h2/a/@title').extract_first(),\n",
        "                'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
        "                'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
        "                'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
        "            }\n",
        "        # Get the URL of the previous page.\n",
        "        next_page = response.xpath('//div[@class=\"nav-previous\"]/a/@href').extract_first()\n",
        "        \n",
        "        # There are a LOT of pages here.  For our example, we'll just scrape the first 9.\n",
        "        # This finds the page number. The next segment of code prevents us from going beyond page 9.\n",
        "        pagenum = int(re.findall(r'\\d+',next_page)[0])\n",
        "        \n",
        "        # Recursively call the spider to run on the next page, if it exists.\n",
        "        if next_page is not None and pagenum < 10:\n",
        "            next_page = response.urljoin(next_page)\n",
        "            # Request the next page and recursively parse it the same way we did above\n",
        "            yield scrapy.Request(next_page, callback=self.parse)\n",
        "\n",
        "# Tell the script how to run the crawler by passing in settings.\n",
        "# The new settings have to do with scraping etiquette.          \n",
        "process = CrawlerProcess({\n",
        "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
        "    'FEED_URI': 'data.json',       # Name our storage file.\n",
        "    'LOG_ENABLED': False,          # Turn off logging for now.\n",
        "    'ROBOTSTXT_OBEY': True,\n",
        "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
        "    'AUTOTHROTTLE_ENABLED': True,\n",
        "    'HTTPCACHE_ENABLED': True\n",
        "})\n",
        "\n",
        "# Start the crawler with our spider.\n",
        "process.crawl(ESSpider)\n",
        "process.start()\n",
        "print('Success!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scrapy/extensions/feedexport.py:210: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
            "  exporter = cls(crawler)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "id": "bjSKQnYZ-zKF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "99aebd42-33eb-4832-fabd-f356282af4ba"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Checking whether we got data from all 9 pages\n",
        "ESSdf=pd.read_json('data.json', orient='records')\n",
        "print(ESSdf.shape)\n",
        "print(ESSdf.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90, 4)\n",
            "        name  ...                             tags\n",
            "0  anonymous  ...                         [School]\n",
            "1    Georgia  ...                               []\n",
            "2         Em  ...  [Always on alert, Public space]\n",
            "3       Anon  ...                         [School]\n",
            "4          J  ...           [Public space, School]\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgtzwpoHatUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "id": "Url6QLGl-zKJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Conclusion\n",
        "\n",
        "Nine pages at 10 entries a row gives us 90 rows - looks like we were successful in our scraping!  This is just a start. Scrapy is very flexible and will let you run multiple spiders with all sorts of different abilities.  In the next assignment, we'll talk about some of the options you can use to fine-tune scrapy and how to use your newfound scraping power in a responsible way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J8FhXZeaulw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "4301ccb2-f32a-4ab1-b41c-390119cb4536"
      },
      "source": [
        "# Importing in each cell because of the kernel restarts.\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "\n",
        "class MEFSpider(scrapy.Spider):\n",
        "    # Naming the spider is important if you are running more than one spider of\n",
        "    # this class simultaneously.\n",
        "    name = \"MEF\"\n",
        "    \n",
        "    # URL(s) to start with.\n",
        "    start_urls = ['https://www.meforum.org/press-releases/?year=year&type=6']\n",
        "\n",
        "    # Use XPath to parse the response we get.\n",
        "    def parse(self, response):\n",
        "        \n",
        "        # Iterate over every <article> element on the page.\n",
        "        for article in response.xpath('//tr' ):\n",
        "            \n",
        "            # Yield a dictionary with the values we want.\n",
        "            yield {\n",
        "                # This is the code to choose what we want to extract\n",
        "                # You can modify this with other Xpath expressions to extract other information from the site\n",
        "                # we have new attributes\n",
        "                'article': article.xpath('td/a/text()').extract_first(),\n",
        "                #Check#'date': article.xpath('td').extract_first(),\n",
        "                #'text': article.xpath('/td[1]/a/text()').extract()\n",
        "            }\n",
        "\n",
        "# Tell the script how to run the crawler by passing in settings.\n",
        "process = CrawlerProcess({\n",
        "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
        "    'FEED_URI': 'firstpage.json',  # Name our storage file.\n",
        "    'LOG_ENABLED': False           # Turn off logging for now.\n",
        "})\n",
        "\n",
        "# Start the crawler with our spider.\n",
        "process.crawl(MEFSpider)\n",
        "process.start()\n",
        "print('Success!')\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scrapy/extensions/feedexport.py:210: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
            "  exporter = cls(crawler)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny31fo4Vbrcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scrapy\n",
        "import scrapy.crawler as crawler\n",
        "from multiprocessing import Process, Queue\n",
        "from twisted.internet import reactor\n",
        "\n",
        "# your spider\n",
        "class MEFSpider(scrapy.Spider):\n",
        "    name = \"MEF\"\n",
        "    start_urls = ['https://www.meforum.org/']\n",
        "\n",
        "    def parse(self, response):\n",
        "        for quote in response.css('div.quote'):\n",
        "            print(quote.css('span.text::text').extract_first())\n",
        "\n",
        "\n",
        "# the wrapper to make it run more times\n",
        "def run_spider(spider):\n",
        "    def f(q):\n",
        "        try:\n",
        "            runner = crawler.CrawlerRunner()\n",
        "            deferred = runner.crawl(spider)\n",
        "            deferred.addBoth(lambda _: reactor.stop())\n",
        "            reactor.run()\n",
        "            q.put(None)\n",
        "        except Exception as e:\n",
        "            q.put(e)\n",
        "\n",
        "    q = Queue()\n",
        "    p = Process(target=f, args=(q,))\n",
        "    p.start()\n",
        "    result = q.get()\n",
        "    p.join()\n",
        "\n",
        "    if result is not None:\n",
        "        raise result"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lFthr7Yb6sj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "e7932e71-3f0c-4c52-b2a6-fb3410b617f9"
      },
      "source": [
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "\n",
        "class MEFSpider(scrapy.Spider):\n",
        "    # Naming the spider is important if you are running more than one spider of\n",
        "    # this class simultaneously.\n",
        "    name = \"MEF\"\n",
        "    \n",
        "    # URL(s) to start with.\n",
        "    start_urls = [\n",
        "        'https://www.meforum.org/',\n",
        "    ]\n",
        "\n",
        "    # Use XPath to parse the response we get.\n",
        "    def parse(self, response):\n",
        "        \n",
        "        # Iterate over every <article> element on the page.\n",
        "        for article in response.xpath('//article'):\n",
        "            \n",
        "            # Yield a dictionary with the values we want.\n",
        "            yield {\n",
        "                # This is the code to choose what we want to extract\n",
        "                # You can modify this with other Xpath expressions to extract other information from the site\n",
        "                'name': article.xpath('header/h2/a/@title').extract_first(),\n",
        "                'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
        "                'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
        "                'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
        "            }\n",
        "\n",
        "# Tell the script how to run the crawler by passing in settings.\n",
        "process = CrawlerProcess({\n",
        "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
        "    'FEED_URI': 'firstpage.json',  # Name our storage file.\n",
        "    'LOG_ENABLED': False           # Turn off logging for now.\n",
        "})\n",
        "\n",
        "# Start the crawler with our spider.\n",
        "process.crawl(MEFSpider)\n",
        "process.start()\n",
        "print('Success!')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scrapy/extensions/feedexport.py:210: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
            "  exporter = cls(crawler)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ReactorNotRestartable",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8690ddf877e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Start the crawler with our spider.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMEFSpider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Success!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, stop_after_crawl)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjustPoolsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'REACTOR_THREADPOOL_MAXSIZE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddSystemEventTrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shutdown'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_graceful_stop_reactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \"\"\"\n\u001b[1;32m   1261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m         \u001b[0mReactorBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReactorNotRestartable\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05zH6vfncIBi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "f46e5669-0f0f-4505-d3c0-3f02092d1a3a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Checking whether we got data from all 9 pages\n",
        "MEFdf=pd.read_json('data.json', orient='records')\n",
        "print(MEFdf.shape)\n",
        "print(MEFdf.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90, 4)\n",
            "        name  ...                             tags\n",
            "0  anonymous  ...                         [School]\n",
            "1    Georgia  ...                               []\n",
            "2         Em  ...  [Always on alert, Public space]\n",
            "3       Anon  ...                         [School]\n",
            "4          J  ...           [Public space, School]\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}